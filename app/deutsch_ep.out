Using TensorFlow backend.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:20 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.7379209067593283 (run: 1/10).
INFO: Testing took 0:07:24 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7528952421288263 (run: 1/10).
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:07:34 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.7226264690853347 (run: 2/10).
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7698145771894674 (run: 2/10).
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:07:34 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:16.
Batch   100  of    304.    Elapsed: 0:00:32.
Batch   150  of    304.    Elapsed: 0:00:48.
Batch   200  of    304.    Elapsed: 0:01:04.
Batch   250  of    304.    Elapsed: 0:01:21.
Batch   300  of    304.    Elapsed: 0:01:38.

  Average training loss: 0.79
  Training epoch took: 0:01:39

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.57
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.65
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.34
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.17
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.13
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.92
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.74
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.58
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.30
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.79
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.17
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.22
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.78
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.63
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.59
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.49
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.35
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.59
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.18
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.89
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.89
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.81
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.58
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.82
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.36
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.99
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.17
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.44
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6039742629725158 (run: 3/10).
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.8085228115601947 (run: 3/10).
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:07:34 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.7005522406545289 (run: 4/10).
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:31 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7900548297319538 (run: 4/10).
INFO: Testing took 0:07:36 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:07:36 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.76
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.54
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.31
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.99
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.16
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.51
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.93
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.70
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.60
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.66
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.29
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.95
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.13
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.25
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.81
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.65
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.64
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.52
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.42
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.26
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.77
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.94
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.70
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.58
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.58
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.33
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.15
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.95
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.78
  Training epoch took: 0:01:48

Now Validating.INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:31 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.48418613328620114 (run: 5/10).
INFO: Testing took 0:07:35 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7581173968270742 (run: 5/10).
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:07:34 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.5767857142857143 (run: 6/10).
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7177713196700033 (run: 6/10).
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:07:34 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

  Validation Accuracy: 0.55
  Validation Loss: 0.79
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.59
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.61
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.39
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.77
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.21
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.05
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.89
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.78
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.56
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.79
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.29
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.86
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.19
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.30
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.79
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.63
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.56
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.32
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.17
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.28
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.91
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.80
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.55
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.81
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.28
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.09
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.13
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.41
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.77
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.78
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6240764138663756 (run: 7/10).
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:32 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7721581196581196 (run: 7/10).
INFO: Testing took 0:07:37 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:07:37 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6058407382091593 (run: 8/10).
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6936173295660364 (run: 8/10).
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:07:34 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.55
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.70
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.32
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.93
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.21
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.06
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.95
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.80
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.64
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:31.
Batch   300  of    304.    Elapsed: 0:01:49.

  Average training loss: 0.41
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.89
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:31.
Batch   300  of    304.    Elapsed: 0:01:49.

  Average training loss: 0.19
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.03
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.77
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.54
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.31
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.84
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.16
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.16
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.93
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.77
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.61
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.74
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.33
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 1.08
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.17
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.31
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.76
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.75
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.57INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_year done.
INFO: Training took 0:09:21 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6796195690719018 (run: 9/10).
INFO: Testing took 0:09:26 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7037161744160539 (run: 9/10).
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:07:34 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6037254263060715 (run: 10/10).
INFO: Testing took 0:07:33 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7455896974214348 (run: 10/10).
INFO: Testing took 0:07:33 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:07:33 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 155.35 minute(s).

  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.84
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.34
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.70
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.17
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.04
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.07
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.28
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.95
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.78
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.64
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.66
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.34
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.85
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.19
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.08
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.76
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.55
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.61
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.33
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.79
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.17
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 1.22
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.89
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.85
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.55
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.28
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.85
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.12
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.26
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________

