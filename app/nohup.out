INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 4 ========
Now Training.
Traceback (most recent call last):
  File "bertclf.py", line 366, in <module>
    main()
  File "bertclf.py", line 232, in main
    loss, logits = model(b_input_ids, 
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/modeling_bert.py", line 1260, in forward
    outputs = self.bert(
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/modeling_bert.py", line 740, in forward
    embedding_output = self.embeddings(
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/modeling_bert.py", line 175, in forward
    inputs_embeds = self.word_embeddings(input_ids)
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 112, in forward
    return F.embedding(
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/torch/nn/functional.py", line 1484, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:10 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:19 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:02:19 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:19 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:02:19 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:16.

  Average training loss: 1.05
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.83
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:16.

  Average training loss: 0.70
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.61
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:16.

  Average training loss: 0.48
  Training epoch took: 0:00:32

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.66
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.31
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 0.61
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.01
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.71
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.61
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.52
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.58
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.30
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.66
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.02
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.43
  Validation Loss: 0.89
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.75
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.49
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.33
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.74
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.00
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.40
  Validation Loss: 0.90
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.74
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.45
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.45
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.29
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.95
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.04
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.36
  Validation Loss: 1.01
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.73
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.51
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.78
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.35
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.84
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.99
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.68
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.45
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.70
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.28
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.81
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.04
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.42
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.73
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.46
  Validation Loss: 0.87
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.45
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.85
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.28
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.97
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.00
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.81
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.68
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.38
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.80
  Validation took: 0:00:01

======== Epoch 4 / 4 ========INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:19 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.23
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.87
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.04
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.44
  Validation Loss: 0.97
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.77
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.85
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.49
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.81
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.31
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.84
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.10
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.18
  Validation Loss: 1.11
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.99
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.36
  Validation Loss: 1.05
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.82
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.49
  Validation Loss: 0.90
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.71
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 0.96
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.03
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.85
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.73
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.75
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.49
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.72
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.34
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.67
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 1.02
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 0.94
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.71
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.65
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.45
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.83
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.29
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.56
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.03
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.83
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.74
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.48
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.63
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.34
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.76
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 1.06
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.95
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.81
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.78
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.49
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.69
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.30
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.67
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.02
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.39
  Validation Loss: 1.00
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.73
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.84
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.50
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.33
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.88
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.00
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.49
  Validation Loss: 0.94
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.67
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.74
  Validation took: 0:00:01INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:19 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:02:19 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:21 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:02:21 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 48.11666666666667 minute(s).


======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.39
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.22
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.82
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.03
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.99
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.74
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.90
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.53
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.83
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.38
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.84
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.03
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.71
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.95
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.47
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.75
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.30
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.80
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.10
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.45
  Validation Loss: 1.02
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.89
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.40
  Validation Loss: 0.99
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.63
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 1.05
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.46
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 1.08
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.02
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 0.89
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.73
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 0.92
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.51
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.93
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.33
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.86
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:04 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:15 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:08:15 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:15 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:10 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:08:10 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:16.
Batch   100  of    338.    Elapsed: 0:00:32.
Batch   150  of    338.    Elapsed: 0:00:48.
Batch   200  of    338.    Elapsed: 0:01:04.
Batch   250  of    338.    Elapsed: 0:01:21.
Batch   300  of    338.    Elapsed: 0:01:38.

  Average training loss: 0.75
  Training epoch took: 0:01:51

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.65
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.56
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.59
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.33
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.61
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.17
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.72
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.88
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.53
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.63
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:12.
Batch   250  of    338.    Elapsed: 0:01:30.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.29
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.14
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.78
  Validation Loss: 0.76
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.78
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.66
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.61
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.61
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.38
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.22
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.75
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.92
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.57
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.78
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.30
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.75
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:26.
Batch   300  of    338.    Elapsed: 0:01:44.

  Average training loss: 0.15
  Training epoch took: 0:01:57

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 0.88
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.INFO: Training for epoch_year done.
INFO: Training took 0:08:15 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:08:13 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:14 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:14 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:08:14 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']


  Average training loss: 0.77
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.55
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.32
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.73
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.16
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.82
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.94
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.60
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.32
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.70
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:44.

  Average training loss: 0.15
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.76
  Validation Loss: 0.81
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.75
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.54
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.53
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.31
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.63
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.15
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.79
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.92
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.58
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.66
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.29
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.83
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.13
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.90
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.76
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.INFO: Training for epoch_year done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:15 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:08:15 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:16 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:14 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:08:14 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.53
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.32
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.84
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.16
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.99
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.92
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.49
  Validation Loss: 0.93
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.57
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.30
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:12.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.14
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.96
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.77
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.65
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.56
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.59
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.35
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.73
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.17
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 1.04
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.92
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.77
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.57
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.70
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.28
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.91
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.11
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.02
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.77
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.70
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.58
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.61
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.INFO: Training for epoch_year done.
INFO: Training took 0:08:15 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:15 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:08:15 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:08:13 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.36
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.88
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.21
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.86
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.93
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.74
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.58
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.63
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.30
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.74
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.15
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.91
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.75
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.54
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.58
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.33
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.17
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.88
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.89
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.81
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.53
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.28
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.78
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.13
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.96
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.78
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.58
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.37
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 4 / 4 ========INFO: Training for epoch_year done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:12 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:08:12 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:16 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:14 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:08:14 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 166.86666666666667 minute(s).

Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.21
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.77
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.88
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.53
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.75
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.28
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.05
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.11
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.31
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.75
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.53
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.63
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.29
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.15
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.85
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.88
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.75
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.54
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.27
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.92
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.12
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.11
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/tokenization_utils.py:884: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.
  warnings.warn(
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:10 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:19 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:02:19 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:16.

  Average training loss: 1.02
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.78
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:16.

  Average training loss: 0.69
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.66
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:16.

  Average training loss: 0.44
  Training epoch took: 0:00:32

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.66
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.24
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 0.61
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.02
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.71
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.60
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.46
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.55
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.26
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.79
  Validation Loss: 0.52
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.96
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.62
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.84
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.35
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.92
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.19
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 1.02
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.98
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.61
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.32
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.83
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.16
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.85
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.00
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.69
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.84
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.42
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.25
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.82
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.98
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 0.89
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.66
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.74
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.40
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.87
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.23
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.75
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.02
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.67
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.78
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.39
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.90
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.22
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.92
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.98
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.84
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.67
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.80
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.49
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.76
  Validation took: 0:00:01

======== Epoch 4 / 4 ========INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:19 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:19 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.32
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.80
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.97
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.80
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.59
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.73
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.33
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.75
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.18
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.78
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.02
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 0.94
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.71
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.50
  Validation Loss: 0.80
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.43
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.74
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.28
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.76
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.98
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.75
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.66
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.69
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.39
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.67
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.21
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.71
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.97
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.82
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.66
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.70
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.39
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.70
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.21
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.70
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.01
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.74
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.69
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.45
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.69
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.28
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.65
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.00
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.75
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.63
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.70
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.40
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.76
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.23
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.81
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.99
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.81
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.61
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.73
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.36
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.71
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.19
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.80
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.04
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.38
  Validation Loss: 0.90
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.72
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.50
  Validation Loss: 0.84
  Validation took: 0:00:01INFO: Training for epoch_poet done.
INFO: Training took 0:02:19 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:02:19 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:19 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:02:19 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 48.36666666666667 minute(s).


======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.48
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.28
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.89
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.03
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.36
  Validation Loss: 0.98
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.71
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.82
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.43
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.27
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.82
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.01
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.35
  Validation Loss: 0.96
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.63
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.95
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.33
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 1.04
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.16
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 1.10
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.98
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.61
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 1.00
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.35
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 1.05
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.21
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 1.08
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.02
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.50
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.65
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.42
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.82
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.25
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.88
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/tokenization_utils.py:884: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.
  warnings.warn(
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:03 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:12 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:08:12 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:04 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:12 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:08:12 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:16.
Batch   100  of    338.    Elapsed: 0:00:31.
Batch   150  of    338.    Elapsed: 0:00:47.
Batch   200  of    338.    Elapsed: 0:01:04.
Batch   250  of    338.    Elapsed: 0:01:21.
Batch   300  of    338.    Elapsed: 0:01:37.

  Average training loss: 0.74
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.48
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.22
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.84
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.10
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.97
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.89
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.47
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.20
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.81
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.08
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.99
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:51.
Batch   200  of    338.    Elapsed: 0:01:08.
Batch   250  of    338.    Elapsed: 0:01:26.
Batch   300  of    338.    Elapsed: 0:01:43.

  Average training loss: 0.76
  Training epoch took: 0:01:56

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.51
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:26.
Batch   300  of    338.    Elapsed: 0:01:44.

  Average training loss: 0.28
  Training epoch took: 0:01:57

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.78
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:26.
Batch   300  of    338.    Elapsed: 0:01:44.

  Average training loss: 0.12
  Training epoch took: 0:01:56

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.03
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:44.

  Average training loss: 0.86
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.66
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.49
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.58
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.23
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.10
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.90
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.INFO: Training for epoch_year done.
INFO: Training took 0:08:12 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:15 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:08:15 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:14 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:12 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:08:12 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']


  Average training loss: 0.74
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.48
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:12.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.27
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.77
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.12
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.96
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.87
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.49
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:12.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.23
  Training epoch took: 0:02:01

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.10
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.76
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.75
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.61
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.48
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.54
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:12.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.23
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.77
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.10
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.87
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.89
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.51
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.60
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.23
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.77
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:44.

  Average training loss: 0.09
  Training epoch took: 0:01:57

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.91
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.74
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.INFO: Training for epoch_year done.
INFO: Training took 0:08:16 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:14 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:08:14 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:11 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:08:13 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.48
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.75
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:12.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.23
  Training epoch took: 0:02:01

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.92
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.10
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.06
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.90
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.51
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.27
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.88
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.13
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.02
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.72
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.60
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.46
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.22
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.89
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.09
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.10
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.88
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.49
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.23
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.91
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.10
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.00
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:26.
Batch   300  of    338.    Elapsed: 0:01:44.

  Average training loss: 0.73
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.63
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:12.
Batch   250  of    338.    Elapsed: 0:01:30.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.47
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.INFO: Training for epoch_year done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:08:13 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:08:13 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.24
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.94
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.09
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.03
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.87
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.73
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.47
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.23
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.00
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.09
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.15
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.74
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.63
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.48
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.79
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.23
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 1.03
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.10
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.19
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.86
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.48
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.22
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.04
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.10
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.14
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.72
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.60
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.45
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.54
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.21
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.73
  Validation took: 0:00:04

======== Epoch 4 / 4 ========INFO: Training for epoch_year done.
INFO: Training took 0:08:15 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:08:13 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:12 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:08:13 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 166.8 minute(s).

Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.08
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.87
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.87
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.78
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.47
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.83
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.23
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.04
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.08
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.23
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.75
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.79
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.50
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.26
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.80
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:44.

  Average training loss: 0.13
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.17
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.89
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:44.

  Average training loss: 0.50
  Training epoch took: 0:01:57

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.75
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.23
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.00
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.10
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.13
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________

  File "bertclf.py", line 319
    if utils.early_stopping(validation_losses, patience=2)
                                                         ^
SyntaxError: invalid syntax
