INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 4 ========
Now Training.
Traceback (most recent call last):
  File "bertclf.py", line 366, in <module>
    main()
  File "bertclf.py", line 232, in main
    loss, logits = model(b_input_ids, 
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/modeling_bert.py", line 1260, in forward
    outputs = self.bert(
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/modeling_bert.py", line 740, in forward
    embedding_output = self.embeddings(
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/modeling_bert.py", line 175, in forward
    inputs_embeds = self.word_embeddings(input_ids)
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 112, in forward
    return F.embedding(
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/torch/nn/functional.py", line 1484, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:10 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:19 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:02:19 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:19 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:02:19 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:16.

  Average training loss: 1.05
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.83
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:16.

  Average training loss: 0.70
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.61
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:16.

  Average training loss: 0.48
  Training epoch took: 0:00:32

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.66
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.31
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 0.61
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.01
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.71
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.61
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.52
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.58
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.30
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.66
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.02
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.43
  Validation Loss: 0.89
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.75
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.49
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.33
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.74
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.00
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.40
  Validation Loss: 0.90
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.74
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.45
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.45
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.29
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.95
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.04
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.36
  Validation Loss: 1.01
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.73
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.51
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.78
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.35
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.84
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.99
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.68
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.45
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.70
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.28
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.81
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.04
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.42
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.73
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.46
  Validation Loss: 0.87
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.45
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.85
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.28
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.97
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.00
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.81
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.68
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.38
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.80
  Validation took: 0:00:01

======== Epoch 4 / 4 ========INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:19 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.23
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.87
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.04
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.44
  Validation Loss: 0.97
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.77
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.85
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.49
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.81
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.31
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.84
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.10
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.18
  Validation Loss: 1.11
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.99
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.36
  Validation Loss: 1.05
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.82
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.49
  Validation Loss: 0.90
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.71
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 0.96
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.03
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.85
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.73
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.75
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.49
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.72
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.34
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.67
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 1.02
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 0.94
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.71
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.65
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.45
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.83
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.29
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.56
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.03
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.83
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.74
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.48
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.63
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.34
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.76
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 1.06
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.95
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.81
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.78
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.49
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.69
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.30
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.67
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.02
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.39
  Validation Loss: 1.00
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.73
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.84
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.50
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.33
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.88
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.00
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.49
  Validation Loss: 0.94
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.67
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.74
  Validation took: 0:00:01INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:19 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:02:19 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:21 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:02:21 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 48.11666666666667 minute(s).


======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.39
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.22
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.82
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.03
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.99
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.74
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.90
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.53
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.83
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.38
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.84
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.03
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.71
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.95
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.47
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.75
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.30
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.80
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.10
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.45
  Validation Loss: 1.02
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.89
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.40
  Validation Loss: 0.99
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.63
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 1.05
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.46
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 1.08
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.02
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 0.89
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.73
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 0.92
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.51
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.93
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.33
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.86
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:04 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:15 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:08:15 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:15 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:10 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:08:10 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:16.
Batch   100  of    338.    Elapsed: 0:00:32.
Batch   150  of    338.    Elapsed: 0:00:48.
Batch   200  of    338.    Elapsed: 0:01:04.
Batch   250  of    338.    Elapsed: 0:01:21.
Batch   300  of    338.    Elapsed: 0:01:38.

  Average training loss: 0.75
  Training epoch took: 0:01:51

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.65
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.56
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.59
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.33
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.61
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.17
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.72
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.88
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.53
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.63
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:12.
Batch   250  of    338.    Elapsed: 0:01:30.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.29
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.14
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.78
  Validation Loss: 0.76
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.78
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.66
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.61
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.61
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.38
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.22
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.75
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.92
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.57
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.78
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.30
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.75
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:26.
Batch   300  of    338.    Elapsed: 0:01:44.

  Average training loss: 0.15
  Training epoch took: 0:01:57

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 0.88
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.INFO: Training for epoch_year done.
INFO: Training took 0:08:15 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:08:13 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:14 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:14 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:08:14 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']


  Average training loss: 0.77
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.55
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.32
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.73
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.16
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.82
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.94
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.60
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.32
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.70
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:44.

  Average training loss: 0.15
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.76
  Validation Loss: 0.81
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.75
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.54
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.53
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.31
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.63
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.15
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.79
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.92
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.58
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.66
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.29
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.83
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.13
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.90
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.76
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.INFO: Training for epoch_year done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:15 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:08:15 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:16 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:14 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:08:14 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.53
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.32
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.84
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.16
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.99
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.92
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.49
  Validation Loss: 0.93
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.57
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.30
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:12.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.14
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.96
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.77
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.65
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.56
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.59
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.35
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.73
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.17
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 1.04
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.92
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.77
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.57
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.70
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.28
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.91
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.11
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.02
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.77
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.70
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.58
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.61
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.INFO: Training for epoch_year done.
INFO: Training took 0:08:15 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:15 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:08:15 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:08:13 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.36
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.88
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.21
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.86
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.93
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.74
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.58
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.63
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.30
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.74
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.15
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.91
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.75
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.54
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.58
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.33
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.17
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.88
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.89
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.81
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.53
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.28
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.78
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.13
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.96
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.78
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.58
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.37
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 4 / 4 ========INFO: Training for epoch_year done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:12 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:08:12 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:16 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:14 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:08:14 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 166.86666666666667 minute(s).

Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.21
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.77
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.88
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.53
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.75
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.28
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.05
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.11
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.31
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.75
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.53
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.63
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.29
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.15
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.85
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.88
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.75
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.54
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.27
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.92
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.12
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.11
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/tokenization_utils.py:884: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.
  warnings.warn(
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:10 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:19 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:02:19 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:16.

  Average training loss: 1.02
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.78
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:16.

  Average training loss: 0.69
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.66
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:16.

  Average training loss: 0.44
  Training epoch took: 0:00:32

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.66
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.24
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 0.61
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.02
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.71
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.60
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.46
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.55
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.26
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.79
  Validation Loss: 0.52
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.96
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.62
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.84
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.35
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.92
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.19
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 1.02
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.98
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.61
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.32
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.83
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.16
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.85
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.00
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.69
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.84
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.42
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.25
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.82
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.98
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 0.89
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.66
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.74
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.40
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.87
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.23
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.75
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.02
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.67
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.78
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.39
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.90
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.22
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.92
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.98
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.84
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.67
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.80
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.49
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.76
  Validation took: 0:00:01

======== Epoch 4 / 4 ========INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:19 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:19 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.32
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.80
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.97
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.80
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.59
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.73
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.33
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.75
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.18
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.78
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.02
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 0.94
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.71
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.50
  Validation Loss: 0.80
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.43
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.74
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.28
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.76
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.98
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.75
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.66
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.69
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.39
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.67
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.21
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.71
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.97
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.82
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.66
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.70
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.39
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.70
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.21
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.70
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.01
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.74
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.69
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.45
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.69
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.28
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.65
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.00
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.75
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.63
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.70
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.40
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.76
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.23
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.81
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.99
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.81
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.61
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.73
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.36
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.71
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.19
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.80
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.04
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.38
  Validation Loss: 0.90
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.72
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.50
  Validation Loss: 0.84
  Validation took: 0:00:01INFO: Training for epoch_poet done.
INFO: Training took 0:02:19 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:02:19 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:02:20 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:02:20 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:02:19 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:02:19 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 48.36666666666667 minute(s).


======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.48
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.28
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.89
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.03
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.36
  Validation Loss: 0.98
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.71
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.82
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.43
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.27
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.82
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.01
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.35
  Validation Loss: 0.96
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.63
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.95
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.33
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 1.04
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.16
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 1.10
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 0.98
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.61
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 1.00
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.35
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 1.05
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.21
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 1.08
  Validation took: 0:00:01
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:17.

  Average training loss: 1.02
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.50
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.65
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.42
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.82
  Validation took: 0:00:01

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of     96.    Elapsed: 0:00:18.

  Average training loss: 0.25
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.88
  Validation took: 0:00:01
--------------------------------

________________________________
________________________________

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/tokenization_utils.py:884: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.
  warnings.warn(
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:03 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:12 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:08:12 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:04 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:12 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:08:12 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:16.
Batch   100  of    338.    Elapsed: 0:00:31.
Batch   150  of    338.    Elapsed: 0:00:47.
Batch   200  of    338.    Elapsed: 0:01:04.
Batch   250  of    338.    Elapsed: 0:01:21.
Batch   300  of    338.    Elapsed: 0:01:37.

  Average training loss: 0.74
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.48
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.22
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.84
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.10
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.97
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.89
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.47
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.20
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.81
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.08
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.99
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:51.
Batch   200  of    338.    Elapsed: 0:01:08.
Batch   250  of    338.    Elapsed: 0:01:26.
Batch   300  of    338.    Elapsed: 0:01:43.

  Average training loss: 0.76
  Training epoch took: 0:01:56

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.51
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:26.
Batch   300  of    338.    Elapsed: 0:01:44.

  Average training loss: 0.28
  Training epoch took: 0:01:57

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.78
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:26.
Batch   300  of    338.    Elapsed: 0:01:44.

  Average training loss: 0.12
  Training epoch took: 0:01:56

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.03
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:44.

  Average training loss: 0.86
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.66
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.49
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.58
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.23
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.10
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.90
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.INFO: Training for epoch_year done.
INFO: Training took 0:08:12 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:15 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:08:15 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:14 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:12 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:08:12 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']


  Average training loss: 0.74
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.48
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:12.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.27
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.77
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.12
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.96
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.87
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.49
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:12.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.23
  Training epoch took: 0:02:01

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.10
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.76
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.75
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.61
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.48
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.54
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:12.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.23
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.77
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.10
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.87
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.89
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.51
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.60
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.23
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.77
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:44.

  Average training loss: 0.09
  Training epoch took: 0:01:57

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.91
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.74
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.INFO: Training for epoch_year done.
INFO: Training took 0:08:16 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:14 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:08:14 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:11 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:08:13 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.48
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.75
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:12.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.23
  Training epoch took: 0:02:01

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.92
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.10
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.06
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.90
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.51
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.27
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.88
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.13
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.02
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.72
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.60
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.46
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.22
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.89
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.09
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.10
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.88
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.49
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.23
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.91
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.10
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.00
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:26.
Batch   300  of    338.    Elapsed: 0:01:44.

  Average training loss: 0.73
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.63
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:12.
Batch   250  of    338.    Elapsed: 0:01:30.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.47
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.INFO: Training for epoch_year done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:08:13 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:08:13 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.24
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.94
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.09
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.03
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.87
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.73
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.47
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.23
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.00
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.09
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.15
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.74
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.63
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.48
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.79
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.23
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 1.03
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.10
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.19
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.86
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.48
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.22
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.04
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.10
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.14
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.72
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.60
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.45
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.54
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.21
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.73
  Validation took: 0:00:04

======== Epoch 4 / 4 ========INFO: Training for epoch_year done.
INFO: Training took 0:08:15 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:08:13 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_year done.
INFO: Training took 0:08:12 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:08:13 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:08:13 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 166.8 minute(s).

Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.08
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.87
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.87
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.78
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.47
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.83
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.23
  Training epoch took: 0:01:59

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.04
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.08
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.23
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:34.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.75
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.79
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.50
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:46.

  Average training loss: 0.26
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.80
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:44.

  Average training loss: 0.13
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.17
  Validation took: 0:00:04
--------------------------------


======== Epoch 1 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:10.
Batch   250  of    338.    Elapsed: 0:01:28.
Batch   300  of    338.    Elapsed: 0:01:45.

  Average training loss: 0.89
  Training epoch took: 0:01:58

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 2 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:17.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:52.
Batch   200  of    338.    Elapsed: 0:01:09.
Batch   250  of    338.    Elapsed: 0:01:27.
Batch   300  of    338.    Elapsed: 0:01:44.

  Average training loss: 0.50
  Training epoch took: 0:01:57

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.75
  Validation took: 0:00:04

======== Epoch 3 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:35.
Batch   150  of    338.    Elapsed: 0:00:53.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.23
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.00
  Validation took: 0:00:04

======== Epoch 4 / 4 ========
Now Training.
Batch    50  of    338.    Elapsed: 0:00:18.
Batch   100  of    338.    Elapsed: 0:00:36.
Batch   150  of    338.    Elapsed: 0:00:54.
Batch   200  of    338.    Elapsed: 0:01:11.
Batch   250  of    338.    Elapsed: 0:01:29.
Batch   300  of    338.    Elapsed: 0:01:47.

  Average training loss: 0.10
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.13
  Validation took: 0:00:04
--------------------------------

________________________________
________________________________

  File "bertclf.py", line 319
    if utils.early_stopping(validation_losses, patience=2)
                                                         ^
SyntaxError: invalid syntax
Using TensorFlow backend.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:13 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: Testing took 0:07:17 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:23 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: Testing took 0:07:27 (h:mm:ss) 

INFO: CV Test F1-Score: 0.7013056631477683
INFO: Training for run 1/10 completed.
INFO: Training run took 0:07:27 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:19 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: Testing took 0:07:23 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:23 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: Testing took 0:07:28 (h:mm:ss) 

INFO: CV Test F1-Score: 0.6783575099364573
INFO: Training for run 2/10 completed.
INFO: Training run took 0:07:28 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:16.
Batch   100  of    304.    Elapsed: 0:00:32.
Batch   150  of    304.    Elapsed: 0:00:48.
Batch   200  of    304.    Elapsed: 0:01:04.
Batch   250  of    304.    Elapsed: 0:01:20.
Batch   300  of    304.    Elapsed: 0:01:37.

  Average training loss: 0.78
  Training epoch took: 0:01:38

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:09.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:44.

  Average training loss: 0.55
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.66
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.31
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.91
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.15
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.48
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 1.01
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.81
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.72
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.44
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.77
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.24
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.85
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.79
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.62
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:44.

  Average training loss: 0.42
  Training epoch took: 0:01:45

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.75
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:09.
Batch   250  of    304.    Elapsed: 0:01:26.
Batch   300  of    304.    Elapsed: 0:01:44.

  Average training loss: 0.24
  Training epoch took: 0:01:45

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.90
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.95
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.80
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.61
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.33
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.90
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.18
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.23
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:53.INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:23 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: Testing took 0:07:28 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:32 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: Testing took 0:07:37 (h:mm:ss) 

INFO: CV Test F1-Score: 0.7167348733138206
INFO: Training for run 3/10 completed.
INFO: Training run took 0:07:37 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:24 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: Testing took 0:07:29 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:20 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: Testing took 0:07:25 (h:mm:ss) 

INFO: CV Test F1-Score: 0.6629243563454089
INFO: Training for run 4/10 completed.
INFO: Training run took 0:07:25 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.78
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.58
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.61
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.36
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.90
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.17
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 1.20
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.90
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.73
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.57
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.74
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.34
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.87
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:31.
Batch   300  of    304.    Elapsed: 0:01:52.

  Average training loss: 0.15
  Training epoch took: 0:01:53

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.26
  Validation took: 0:00:05
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.77
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.50
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.59
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.61
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.34
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.20
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.87
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.92
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.85
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.55
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.78
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.33
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.93
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.15
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.45
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.79
  Training epoch took: 0:01:46

Now Validating.INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_year done.
INFO: Training took 0:09:16 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: Testing took 0:09:21 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:33 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: Testing took 0:07:38 (h:mm:ss) 

INFO: CV Test F1-Score: 0.6411021239968608
INFO: Training for run 5/10 completed.
INFO: Training run took 0:07:38 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: Testing took 0:07:33 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: CV Test F1-Score: 0.6550505050505051
INFO: Training for run 6/10 completed.
INFO: Training run took 0:07:34 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

  Validation Accuracy: 0.60
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.62
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.43
  Validation Loss: 0.75
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.41
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.60
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.24
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.97
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.14
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.03
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.91
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.82
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.59
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.66
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:31.
Batch   300  of    304.    Elapsed: 0:01:49.

  Average training loss: 0.34
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 1.43
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.15
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.83
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.77
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.65
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.56
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.57
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.33
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.74
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.18
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.93
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.93
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.57
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.63
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.29
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.94
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.14
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.42
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: CV Test F1-Score: 0.6506892230576442
INFO: Training for run 7/10 completed.
INFO: Training run took 0:07:34 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:32 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: Testing took 0:07:36 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:35 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: Testing took 0:07:40 (h:mm:ss) 

INFO: CV Test F1-Score: 0.719667223614592
INFO: Training for run 8/10 completed.
INFO: Training run took 0:07:40 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.75
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.44
  Validation Loss: 0.80
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.54
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.31
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 1.26
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.18
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 1.45
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.92
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.80
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.57
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.81
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.30
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 1.24
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.13
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 2.07
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.77
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.53
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.32
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.98
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:13.
Batch   250  of    304.    Elapsed: 0:01:31.
Batch   300  of    304.    Elapsed: 0:01:49.

  Average training loss: 0.17
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.30
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.91
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:55.
Batch   200  of    304.    Elapsed: 0:01:13.
Batch   250  of    304.    Elapsed: 0:01:31.
Batch   300  of    304.    Elapsed: 0:01:49.

  Average training loss: 0.58
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.73
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:55.
Batch   200  of    304.    Elapsed: 0:01:13.
Batch   250  of    304.    Elapsed: 0:01:31.
Batch   300  of    304.    Elapsed: 0:01:49.

  Average training loss: 0.34
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.87
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:31.
Batch   300  of    304.    Elapsed: 0:01:49.

  Average training loss: 0.17
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 0.91
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.75INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: Testing took 0:07:33 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:22 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: Testing took 0:07:26 (h:mm:ss) 

INFO: CV Test F1-Score: 0.6015675357780621
INFO: Training for run 9/10 completed.
INFO: Training run took 0:07:26 (h:mm:ss)
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_year done.
INFO: Training took 0:09:10 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: Testing took 0:09:14 (h:mm:ss) 

INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt from cache at /home/paulus/.cache/torch/transformers/31cabea4c0b209d88980a7ce42042f9b2814074a57b22656b68f82598787f1df.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json from cache at /home/paulus/.cache/torch/transformers/0dfefbc876c3d95e5e69d083ef59737a83cdca73434cef0b055c065217c5c132.20a95e3fb07c002de6bb9b1aeb3b00e9cb3ce70a1ed7c27f66f41ecb3e607ad1
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/cfd096fadf5d8e41a15715893a5085ca5af611a3cb6521b696c4b680077e1da9.9e54461b318575a272338a079389939660145c8a016316a649b1e9226d670962
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:22 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: Testing took 0:07:27 (h:mm:ss) 

INFO: CV Test F1-Score: 0.6628344852029063
INFO: Training for run 10/10 completed.
INFO: Training run took 0:07:27 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 156.03333333333333 minute(s).

  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.52
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 0.75
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.31
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.00
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.14
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.27
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.91
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.75
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.59
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.33
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.02
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.17
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.54
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:09.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:44.

  Average training loss: 0.82
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.75
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.71
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.49
  Validation Loss: 0.75
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.57
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.38
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.85
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.29
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.22
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.91
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.65
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.54
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.27
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.93
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.13
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.25
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________

Using TensorFlow backend.
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/tokenization_utils.py:884: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.
  warnings.warn(
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:01:55 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6914972233154051 (run: 1/10).
INFO: Testing took 0:01:57 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:35 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7707792207792207 (run: 1/10).
INFO: Testing took 0:02:36 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:02:36 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 7).
INFO: Training for epoch_year done.
INFO: Training took 0:04:10 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.556093401547947 (run: 2/10).
INFO: Testing took 0:04:12 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:05 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5288600288600288 (run: 2/10).
INFO: Testing took 0:02:06 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:02:06 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_year done.
INFO: Training took 0:02:36 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.5953583453583452 (run: 3/10).
INFO: Testing took 0:02:37 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:05 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5600135554681008 (run: 3/10).
INFO: Testing took 0:02:06 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:02:06 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:16.

  Average training loss: 1.00
  Training epoch took: 0:00:27

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.91
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:16.

  Average training loss: 0.68
  Training epoch took: 0:00:27

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:16.

  Average training loss: 0.39
  Training epoch took: 0:00:28

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.97
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.25
  Training epoch took: 0:00:29

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 1.33
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.02
  Training epoch took: 0:00:29

Now Validating.
  Validation Accuracy: 0.38
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.72
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.62
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.41
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.60
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.20
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.77
  Validation Loss: 0.68
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.05
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.03
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.00
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.83
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.66
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.69
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.37
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.82
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.17
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.05
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 1.72
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.01
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 1.34
  Validation took: 0:00:01

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.02
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.54
  Validation took: 0:00:01

======== Epoch 8 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.00
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.61
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.98
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.32
  Validation Loss: 1.05
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.70
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.95
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.43
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 1.08
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.22
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 1.18
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.01
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.41
  Validation Loss: 0.90
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.66
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.49
  Validation Loss: 0.87
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.44
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.76
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.19
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 1.05
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.09
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 1.21
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.00
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.67
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.49
  Validation Loss: 0.80
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.38
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.83
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.18
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 1.02
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:02:05 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.5584415584415585 (run: 4/10).
INFO: Testing took 0:02:06 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:05 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.675877825877826 (run: 4/10).
INFO: Testing took 0:02:06 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:02:06 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_year done.
INFO: Training took 0:02:36 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6138637456819274 (run: 5/10).
INFO: Testing took 0:02:38 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:37 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6761423761423762 (run: 5/10).
INFO: Testing took 0:02:38 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:02:38 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:02:05 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6027417027417027 (run: 6/10).
INFO: Testing took 0:02:06 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:36 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7653679653679654 (run: 6/10).
INFO: Testing took 0:02:37 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:02:37 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']


  Average training loss: 1.01
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.87
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.69
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.50
  Validation Loss: 0.87
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.41
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.89
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.18
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 1.35
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.00
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.85
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.68
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.72
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.38
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.73
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.20
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.79
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.01
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.71
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.67
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.44
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.64
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.22
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.95
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.15
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 1.35
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.00
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.45
  Validation Loss: 0.94
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.66
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.87
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.41
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.78
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.19
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 1.06
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.07
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.17
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.02
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.37
  Validation Loss: 1.00
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.76
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.46
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 1.20
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.29
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.50
  Validation Loss: 1.43
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.99
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.70
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.76
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.48
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.61
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.27
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.73
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.11
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.01
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.05
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 0.83
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.71
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.41
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.68
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.21
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.82INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_year done.
INFO: Training took 0:02:36 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6408260090078272 (run: 7/10).
INFO: Testing took 0:02:38 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 8).
INFO: Training for epoch_poet done.
INFO: Training took 0:04:41 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6692881192881194 (run: 7/10).
INFO: Testing took 0:04:42 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:04:42 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:02:05 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.5132034632034631 (run: 8/10).
INFO: Testing took 0:02:06 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:05 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5906685906685906 (run: 8/10).
INFO: Testing took 0:02:06 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:02:06 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:02:05 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6915103415103414 (run: 9/10).
INFO: Testing took 0:02:07 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:05 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5822510822510822 (run: 9/10).
INFO: Testing took 0:02:07 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:02:07 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.08
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.29
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.05
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.76
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.69
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.64
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.39
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.71
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.23
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.82
  Validation Loss: 0.54
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.05
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.83
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.02
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.80
  Validation Loss: 0.80
  Validation took: 0:00:01

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.00
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.79
  Validation Loss: 0.73
  Validation took: 0:00:01

======== Epoch 8 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.00
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.80
  Validation Loss: 0.74
  Validation took: 0:00:01

======== Epoch 9 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.00
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.80
  Validation Loss: 0.79
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.02
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.42
  Validation Loss: 0.93
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.67
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.89
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.34
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 1.07
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.17
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 1.39
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.98
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.50
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.66
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.63
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.36
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.72
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.19
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.90
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.99
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.45
  Validation Loss: 0.85
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.65
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.37
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.50
  Validation Loss: 1.06
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.20
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.46
  Validation Loss: 1.39
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.98
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.50
  Validation Loss: 0.96
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.68
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.76
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.36
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.80
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.14
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.99
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.04
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.36
  Validation Loss: 0.96
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.69
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.82
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.38
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.81
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.20
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 1.09
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_year done.
INFO: Training took 0:02:36 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.46534391534391534 (run: 10/10).
INFO: Testing took 0:02:37 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:37 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6191678691678691 (run: 10/10).
INFO: Testing took 0:02:38 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:02:38 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 52.7 minute(s).


  Average training loss: 0.07
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.35
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.00
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.81
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.69
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.67
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.36
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.60
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.14
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.92
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.04
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.07
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________

Using TensorFlow backend.
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/tokenization_utils.py:884: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.
  warnings.warn(
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:21 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6106513369671265 (run: 1/10).
INFO: Testing took 0:07:25 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:23 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.693570415938837 (run: 1/10).
INFO: Testing took 0:07:27 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:07:27 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:23 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6377431535326271 (run: 2/10).
INFO: Testing took 0:07:27 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:23 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7005113794587478 (run: 2/10).
INFO: Testing took 0:07:27 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:07:27 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:16.
Batch   100  of    304.    Elapsed: 0:00:32.
Batch   150  of    304.    Elapsed: 0:00:49.
Batch   200  of    304.    Elapsed: 0:01:05.
Batch   250  of    304.    Elapsed: 0:01:22.
Batch   300  of    304.    Elapsed: 0:01:39.

  Average training loss: 0.74
  Training epoch took: 0:01:40

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.51
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.82
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.26
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.97
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.13
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.44
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.87
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.53
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.61
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.30
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.82
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.16
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.03
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.76
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.52
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.58
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.29
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.95
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.11
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.21
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.90
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.56
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.28
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.30
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.10
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.32
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:22 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.5917312706786391 (run: 3/10).
INFO: Testing took 0:07:27 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:22 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6771847548163337 (run: 3/10).
INFO: Testing took 0:07:27 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:07:27 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:23 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.623058666479719 (run: 4/10).
INFO: Testing took 0:07:27 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:23 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6675379201694992 (run: 4/10).
INFO: Testing took 0:07:27 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:07:27 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.75
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.53
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.29
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.12
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.23
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.91
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.52
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.58
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.27
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.99
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.12
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.61
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.79
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.55
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.28
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.02
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.13
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 1.49
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.89
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.75
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.51
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.84
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.28
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.01
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.10
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.74
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.76
  Training epoch took: 0:01:46

Now Validating.INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:26 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6320154504365029 (run: 5/10).
INFO: Testing took 0:07:30 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:23 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.689281284018126 (run: 5/10).
INFO: Testing took 0:07:28 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:07:28 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:23 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.612591648117964 (run: 6/10).
INFO: Testing took 0:07:28 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:22 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6749512670565303 (run: 6/10).
INFO: Testing took 0:07:27 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:07:27 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

  Validation Accuracy: 0.53
  Validation Loss: 0.77
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.53
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.28
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 1.06
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.11
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 1.62
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.94
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.57
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.53
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.32
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.79
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.14
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.99
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.76
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.73
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.53
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.75
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.30
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.79
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.13
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 1.24
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.90
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.75
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.49
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.60
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.23
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.07
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.09
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.37
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.73
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.75
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:23 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6066766079923973 (run: 7/10).
INFO: Testing took 0:07:27 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:22 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7018448900027848 (run: 7/10).
INFO: Testing took 0:07:27 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:07:27 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:23 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.622901416322469 (run: 8/10).
INFO: Testing took 0:07:27 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:23 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.675824662666768 (run: 8/10).
INFO: Testing took 0:07:27 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:07:27 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']

Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.48
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.90
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.24
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.99
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.12
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 1.70
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.91
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.56
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.27
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.14
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.23
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.77
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.50
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.79
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.27
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 1.03
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.12
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 1.38
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.90
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.78
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.54
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.27
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.85
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.10
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.39
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.77
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.61
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.53INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:23 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6991557662610294 (run: 9/10).
INFO: Testing took 0:07:27 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:23 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6601173680121049 (run: 9/10).
INFO: Testing took 0:07:28 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:07:28 (h:mm:ss)
INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:23 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6301987973040606 (run: 10/10).
INFO: Testing took 0:07:27 (h:mm:ss) 

INFO: Model name 'redewiedergabe/bert-base-historical-german-rw-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'redewiedergabe/bert-base-historical-german-rw-cased' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/vocab.txt from cache at /home/paulus/.cache/torch/transformers/b25d3ecfe9931049cada1de9f396b9896126fb8c849acb62f690eee8bcd6eb20.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/added_tokens.json from cache at /home/paulus/.cache/torch/transformers/f48b1bb3e7da99e956aa11d68dbccc34fb5523b93319193cb8eb385dc73fc787.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/special_tokens_map.json from cache at /home/paulus/.cache/torch/transformers/f1df45096f88eb4caefd14a1e5e9c5b088a1c050fe911dfbe3909bd4a23c6780.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/tokenizer_config.json from cache at /home/paulus/.cache/torch/transformers/b336aa7b57c73dcca1ba03b20f91abb4f17dcee75e17b7643bea1555610eaa10.85aeda2ee9f5ffe5945714ad1ffe401aaeb83ce6b05c554f972487c55bb1a407
INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/redewiedergabe/bert-base-historical-german-rw-cased/config.json from cache at /home/paulus/.cache/torch/transformers/c7eac2bbd5eeb696e76fe4ab946bf85556fa8cc563a0f45c6b98f1dd62d4e19f.4d552623f46c71e8e0c1a0eb4e59da5816bca7958d32ae9290d9de4cee162c6b
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file https://cdn.huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased/pytorch_model.bin from cache at /home/paulus/.cache/torch/transformers/7b94372da10571827a243ce8a66111e0add5b5488fd5c0e1a11cc9eeda97975f.18e920485dc2653505ff82f986d2f87173ef2c3d2dd9498a93f49e498e46c8da
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:09:14 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7155780866307183 (run: 10/10).
INFO: Testing took 0:09:18 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:09:18 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 153.61666666666667 minute(s).

  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.65
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.28
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.10
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.89
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.87
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.51
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.61
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.25
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.91
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.11
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.38
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.72
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.47
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.25
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 1.22
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.10
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.71
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.88
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.51
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.26
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.70
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.10
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.77
  Validation Loss: 0.92
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.03
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 1.28
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________

Using TensorFlow backend.
INFO: Model name '../corpora/domain-adaption/german/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german/pytorch_model.bin
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti
Traceback (most recent call last):
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/modeling_utils.py", line 673, in from_pretrained
    state_dict = torch.load(resolved_archive_file, map_location="cpu")
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/torch/serialization.py", line 529, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/torch/serialization.py", line 692, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
_pickle.UnpicklingError: invalid load key, 'v'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "bertclf.py", line 464, in <module>
    main()
  File "bertclf.py", line 175, in main
    model = BertForSequenceClassification.from_pretrained(model_name, 
  File "/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/modeling_utils.py", line 675, in from_pretrained
    raise OSError(
OSError: Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. 
Using TensorFlow backend.
INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 8).
INFO: Training for epoch_year done.
INFO: Training took 0:04:35 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6127946127946127 (run: 1/10).
INFO: Testing took 0:04:36 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_poet done.
INFO: Training took 0:03:10 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6201298701298701 (run: 1/10).
INFO: Testing took 0:03:11 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:03:11 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_year done.
INFO: Training took 0:02:38 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.581096681096681 (run: 2/10).
INFO: Testing took 0:02:39 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 8).
INFO: Training for epoch_poet done.
INFO: Training took 0:04:45 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5649547422274694 (run: 2/10).
INFO: Testing took 0:04:46 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:04:46 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:16.

  Average training loss: 1.11
  Training epoch took: 0:00:28

Now Validating.
  Validation Accuracy: 0.15
  Validation Loss: 1.16
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:16.

  Average training loss: 1.11
  Training epoch took: 0:00:28

Now Validating.
  Validation Accuracy: 0.15
  Validation Loss: 1.10
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:16.

  Average training loss: 1.11
  Training epoch took: 0:00:28

Now Validating.
  Validation Accuracy: 0.13
  Validation Loss: 1.12
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.09
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.29
  Validation Loss: 1.09
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.02
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.31
  Validation Loss: 1.05
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.85
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.50
  Validation Loss: 0.89
  Validation took: 0:00:01

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.62
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 8 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.45
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.96
  Validation took: 0:00:01

======== Epoch 9 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.30
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 1.12
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.11
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.35
  Validation Loss: 1.04
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.03
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.44
  Validation Loss: 1.01
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.87
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.46
  Validation Loss: 0.85
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.62
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.75
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.44
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.81
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.28
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.82
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.11
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.23
  Validation Loss: 1.11
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.03
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.39
  Validation Loss: 1.05
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.88
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.45
  Validation Loss: 0.95
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.65
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 0.97
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.42
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 1.03
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.09
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.38
  Validation Loss: 0.99
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.00
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.26
  Validation Loss: 1.10
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.91
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.46
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.71
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.53
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.90
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.44
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.85
  Validation took: 0:00:01

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.32
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.81
  Validation took: 0:00:01

======== Epoch 8 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.19
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.02
  Validation took: 0:00:01

======== Epoch 9 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.12
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.11
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.12
  Training epoch took: 0:00:29

Now Validating.
  Validation Accuracy: 0.35
  Validation Loss: 1.05
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.04
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.37
  Validation Loss: 1.11
  Validation took: 0:00:01

======== Epoch 3 / 10 ========INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_year done.
INFO: Training took 0:02:37 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.4794853294853295 (run: 3/10).
INFO: Testing took 0:02:38 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Training for epoch_poet done.
INFO: Training took 0:05:13 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5332611832611831 (run: 3/10).
INFO: Testing took 0:05:14 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:05:14 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 6).
INFO: Training for epoch_year done.
INFO: Training took 0:03:38 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.546232891687437 (run: 4/10).
INFO: Testing took 0:03:39 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_poet done.
INFO: Training took 0:03:09 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5216515807424899 (run: 4/10).
INFO: Testing took 0:03:10 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:03:10 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']

Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.82
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.46
  Validation Loss: 0.92
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.56
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 1.03
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.41
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 1.15
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.09
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.38
  Validation Loss: 1.10
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.97
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.44
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.79
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.36
  Validation Loss: 1.12
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.59
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.49
  Validation Loss: 0.84
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.37
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.23
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.81
  Validation took: 0:00:01

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.15
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 1.11
  Validation took: 0:00:01

======== Epoch 8 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.05
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.98
  Validation took: 0:00:01

======== Epoch 9 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.03
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.06
  Validation took: 0:00:01

======== Epoch 10 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.01
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.03
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.10
  Training epoch took: 0:00:29

Now Validating.
  Validation Accuracy: 0.27
  Validation Loss: 1.05
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.00
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.40
  Validation Loss: 0.95
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.77
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.83
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.53
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 1.17
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.38
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.97
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.19
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 1.04
  Validation took: 0:00:01

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.08
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 1.40
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.08
  Training epoch took: 0:00:29

Now Validating.
  Validation Accuracy: 0.30
  Validation Loss: 1.11
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.03
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.35
  Validation Loss: 1.06
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.86
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.44
  Validation Loss: 1.00
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.64
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 0.95
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.43
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 1.08
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.28
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.11
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.11
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.39
  Validation Loss: 1.07
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.03
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.41
  Validation Loss: 1.00
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.81
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.49
  Validation Loss: 1.15
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.61
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 1.03
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.42
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 1.19
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.34INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_year done.
INFO: Training took 0:03:07 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.5626240762604399 (run: 5/10).
INFO: Testing took 0:03:08 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:38 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.44309764309764305 (run: 5/10).
INFO: Testing took 0:02:40 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:02:40 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_year done.
INFO: Training took 0:02:38 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.5610630110630112 (run: 6/10).
INFO: Testing took 0:02:39 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 6).
INFO: Training for epoch_poet done.
INFO: Training took 0:03:39 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.593421225239407 (run: 6/10).
INFO: Testing took 0:03:40 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:03:40 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_year done.
INFO: Training took 0:03:12 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.5458087367178277 (run: 7/10).
INFO: Testing took 0:03:13 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 6).
INFO: Training for epoch_poet done.
INFO: Training took 0:03:41 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5318509773055227 (run: 7/10).
INFO: Testing took 0:03:43 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:03:43 (h:mm:ss)

  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 1.27
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.10
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.30
  Validation Loss: 1.09
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.02
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.43
  Validation Loss: 0.95
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.88
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.44
  Validation Loss: 0.85
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.62
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 1.04
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.42
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.46
  Validation Loss: 1.08
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.11
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.16
  Validation Loss: 1.19
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.99
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.37
  Validation Loss: 1.15
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.75
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.50
  Validation Loss: 0.94
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.52
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 1.06
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.37
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.49
  Validation Loss: 1.18
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.10
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.32
  Validation Loss: 1.02
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.04
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.45
  Validation Loss: 0.93
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.87
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.81
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.70
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.83
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.53
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.37
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.85
  Validation took: 0:00:01

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.27
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.49
  Validation Loss: 0.96
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.11
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.30
  Validation Loss: 1.07
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.07
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.34
  Validation Loss: 1.01
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.97
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.49
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.73
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.80
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.50
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.89
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.32
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.89
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.08
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.29
  Validation Loss: 1.05
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.98
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.37
  Validation Loss: 1.05
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.77
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.36
  Validation Loss: 0.98
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.59
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.35
  Validation Loss: 1.13
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.40
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.43
  Validation Loss: 1.06
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.26
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 1.18
  Validation took: 0:00:01

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.15
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.39
  Validation Loss: 1.51
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________
INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_year done.
INFO: Training took 0:03:09 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.5945996764178582 (run: 8/10).
INFO: Testing took 0:03:10 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 7).
INFO: Training for epoch_poet done.
INFO: Training took 0:04:10 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5588263588263588 (run: 8/10).
INFO: Testing took 0:04:11 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:04:11 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_year done.
INFO: Training took 0:02:37 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.5094757094757095 (run: 9/10).
INFO: Testing took 0:02:38 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_poet done.
INFO: Training took 0:03:06 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.46293891293891304 (run: 9/10).
INFO: Testing took 0:03:07 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:03:07 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.12
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.37
  Validation Loss: 1.07
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.03
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 0.95
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.84
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 1.05
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.63
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.81
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.40
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 1.00
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.24
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 1.17
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.10
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.17
  Validation Loss: 1.09
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.06
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.32
  Validation Loss: 1.01
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.86
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.28
  Validation Loss: 1.37
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.73
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.43
  Validation Loss: 1.10
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.58
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.42
  Validation Loss: 1.40
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.43
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 1.35
  Validation took: 0:00:01

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.29
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.46
  Validation Loss: 1.46
  Validation took: 0:00:01

======== Epoch 8 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.18
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 1.59
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.10
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.33
  Validation Loss: 1.06
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.99
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.40
  Validation Loss: 0.98
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.79
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.51
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.97
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.35
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 1.04
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.09
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.35
  Validation Loss: 1.04
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.04
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.46
  Validation Loss: 0.98
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.92
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.90
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.67
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.85
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.55
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.39
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.92
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.11
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.26
  Validation Loss: 1.12
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.00
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.31
  Validation Loss: 1.05
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.81
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.84
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.58
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.45
  Validation Loss: 1.24
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.40
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.95
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.27
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 1.19
  Validation took: 0:00:01

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.INFO: Training for epoch_year done.
INFO: Training took 0:05:17 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.5398377716559536 (run: 10/10).
INFO: Testing took 0:05:18 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:07 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.4531024531024531 (run: 10/10).
INFO: Testing took 0:02:08 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:02:08 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 70.76666666666667 minute(s).


  Average training loss: 0.20
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 1.10
  Validation took: 0:00:01

======== Epoch 8 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.10
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 1.54
  Validation took: 0:00:01

======== Epoch 9 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.09
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 1.52
  Validation took: 0:00:01

======== Epoch 10 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.07
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 1.68
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 1.11
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.32
  Validation Loss: 1.14
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.05
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.35
  Validation Loss: 1.09
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.92
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.34
  Validation Loss: 1.12
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.70
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.41
  Validation Loss: 1.13
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________

Using TensorFlow backend.
INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:16 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.575247851563641 (run: 1/10).
INFO: Testing took 0:07:21 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:30 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5136477557530189 (run: 1/10).
INFO: Testing took 0:07:35 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:07:35 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:27 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.5790245816561604 (run: 2/10).
INFO: Testing took 0:07:32 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:16.
Batch   100  of    304.    Elapsed: 0:00:32.
Batch   150  of    304.    Elapsed: 0:00:48.
Batch   200  of    304.    Elapsed: 0:01:04.
Batch   250  of    304.    Elapsed: 0:01:21.
Batch   300  of    304.    Elapsed: 0:01:38.

  Average training loss: 0.82
  Training epoch took: 0:01:39

Now Validating.
  Validation Accuracy: 0.50
  Validation Loss: 0.73
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.72
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.79
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.59
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.90
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.46
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.98
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 1.05
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.43
  Validation Loss: 0.96
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.86
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.85
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.61
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.95
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.41
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 1.35
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.82
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.71
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.66
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.55
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.43
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.87
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 1.10
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.34
  Validation Loss: 1.03
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.95
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 0.89
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.78
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 1.04
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.61
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.97
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:19.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_poet done.
INFO: Training took 0:11:10 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6295296321612112 (run: 2/10).
INFO: Testing took 0:11:14 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:11:14 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_year done.
INFO: Training took 0:09:17 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.5621472581998898 (run: 3/10).
INFO: Testing took 0:09:22 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:09:15 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5915470494417863 (run: 3/10).
INFO: Testing took 0:09:20 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:09:20 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']


  Average training loss: 0.47
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 1.07
  Validation took: 0:00:04

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.37
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 1.37
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.83
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.71
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.52
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.60
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.35
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.81
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.22
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.06
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 1.04
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.44
  Validation Loss: 1.00
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.88
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.94
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.70
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.94
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.54
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 1.11
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.37
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 1.17
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.82
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 0.81
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.72
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 0.73
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.57
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.73
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.40
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 1.13
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.26
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 1.10
  Validation took: 0:00:04

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.INFO: Stopping epoch run early (Epoch 6).
INFO: Training for epoch_year done.
INFO: Training took 0:13:00 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.5915333204806889 (run: 4/10).
INFO: Testing took 0:13:04 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:09:19 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.637164881901724 (run: 4/10).
INFO: Testing took 0:09:24 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:09:24 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:27 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.5580724830724831 (run: 5/10).
INFO: Testing took 0:07:32 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']

Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.17
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 1.52
  Validation took: 0:00:04

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.10
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 1.66
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 1.02
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.50
  Validation Loss: 0.91
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.83
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.60
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.36
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.19
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.23
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.80
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 0.73
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.71
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.70
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.55
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 0.77
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.40
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.98
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:09.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:44.

  Average training loss: 1.02
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.89
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.83
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.86
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.62
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.89
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.43
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.85
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.24
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.17
  Validation took: 0:00:04

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.14
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 1.59
  Validation took: 0:00:04INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_poet done.
INFO: Training took 0:11:06 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6304220146325409 (run: 5/10).
INFO: Testing took 0:11:10 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:11:10 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_year done.
INFO: Training took 0:09:19 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.5041008698903435 (run: 6/10).
INFO: Testing took 0:09:24 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:09:19 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5557606136553506 (run: 6/10).
INFO: Testing took 0:09:24 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:09:24 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_year done.
INFO: Training took 0:11:09 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6130333604017814 (run: 7/10).
INFO: Testing took 0:11:13 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']

--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.80
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 0.87
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.69
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.50
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.66
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.31
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 1.01
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.17
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 1.28
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 1.06
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.40
  Validation Loss: 1.00
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.92
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.91
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.74
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.90
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.57
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.96
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.37
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 1.20
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.81
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 0.80
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.69
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.84
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.61
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.97
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.48
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.93
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.38
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 1.34
  Validation took: 0:00:04

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.25
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 1.58
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:09.INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:09:11 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5801656920077972 (run: 7/10).
INFO: Testing took 0:09:15 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:09:15 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:31 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.5247374457900773 (run: 8/10).
INFO: Testing took 0:07:35 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:09:23 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6705615047720311 (run: 8/10).
INFO: Testing took 0:09:28 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:09:28 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']

Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:44.

  Average training loss: 1.05
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.43
  Validation Loss: 1.04
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.86
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.89
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.66
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.81
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.47
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.98
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.30
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 1.20
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.82
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.81
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.70
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.54
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.36
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.89
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 1.01
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.49
  Validation Loss: 0.94
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.78
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.90
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.54
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.85
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.34
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.00
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.20
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 1.54
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.82
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.74
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.70
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.63
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 4 / 10 ========INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:28 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.5479026821132083 (run: 9/10).
INFO: Testing took 0:07:33 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:26 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6303637882585251 (run: 9/10).
INFO: Testing took 0:07:31 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:07:31 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.598488889278363 (run: 10/10).
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/german-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/german-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/german-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/german-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/german-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/german-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/german-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 31102
}

INFO: loading weights file ../corpora/domain-adaption/german-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:09:22 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5779765069238753 (run: 10/10).
INFO: Testing took 0:09:27 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:09:27 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 184.1 minute(s).

Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.52
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.79
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 1.03
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 0.93
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.82
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.85
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.57
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.88
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.35
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 1.11
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.80
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.44
  Validation Loss: 0.77
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:37.
Batch   150  of    304.    Elapsed: 0:00:55.
Batch   200  of    304.    Elapsed: 0:01:13.
Batch   250  of    304.    Elapsed: 0:01:32.
Batch   300  of    304.    Elapsed: 0:01:50.

  Average training loss: 0.70
  Training epoch took: 0:01:51

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.53
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.39
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.90
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 1.04
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 0.96
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.84
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.89
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.63
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.79
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:37.
Batch   150  of    304.    Elapsed: 0:00:55.
Batch   200  of    304.    Elapsed: 0:01:13.
Batch   250  of    304.    Elapsed: 0:01:31.
Batch   300  of    304.    Elapsed: 0:01:49.

  Average training loss: 0.45
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.86
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.26
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 1.16
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________

Using TensorFlow backend.
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/tokenization_utils.py:884: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.
  warnings.warn(
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:01:57 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6966198784380603 (run: 1/10).
INFO: Testing took 0:01:59 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:04 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.8738204556386374 (run: 1/10).
INFO: Testing took 0:02:06 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:02:06 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_year done.
INFO: Training took 0:02:36 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.5966417420962875 (run: 2/10).
INFO: Testing took 0:02:37 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:05 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6687459005640825 (run: 2/10).
INFO: Testing took 0:02:07 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:02:07 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_year done.
INFO: Training took 0:02:38 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6565656565656566 (run: 3/10).
INFO: Testing took 0:02:39 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:05 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5915103415103414 (run: 3/10).
INFO: Testing took 0:02:06 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:02:06 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:02:07 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.5951491269673088 (run: 4/10).
INFO: Testing took 0:02:08 (h:mm:ss) 

There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:16.

  Average training loss: 0.98
  Training epoch took: 0:00:27

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.80
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:16.

  Average training loss: 0.58
  Training epoch took: 0:00:28

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.63
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.24
  Training epoch took: 0:00:28

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.06
  Training epoch took: 0:00:29

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.02
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.94
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.75
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.57
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.27
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.97
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.13
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.08
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.94
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.76
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.53
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.73
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.25
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.71
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.09
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.89
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.02
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.08
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.93
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.87
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.52
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.70
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.24
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 1.09
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.09
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.17
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.96
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.74
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.56
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.82
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.27
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.68
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.10
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.05
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.02
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.32
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.95
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.78
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.50
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.50
  Validation Loss: 0.82
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.21
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.01
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.06
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 1.90
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.93
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.91
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.56
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 0.98
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.25
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 1.20
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.10
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 1.73
  Validation took: 0:00:01
--------------------------------

--------------------------------
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:04 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.602116402116402 (run: 4/10).
INFO: Testing took 0:02:06 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:02:06 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:02:07 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6699522699522699 (run: 5/10).
INFO: Testing took 0:02:08 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:07 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6445993400538854 (run: 5/10).
INFO: Testing took 0:02:08 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:02:08 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_year done.
INFO: Training took 0:02:37 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.653030303030303 (run: 6/10).
INFO: Testing took 0:02:39 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:08 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6875180375180375 (run: 6/10).
INFO: Testing took 0:02:09 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:02:09 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:02:05 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.624098124098124 (run: 7/10).
INFO: Testing took 0:02:07 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:05 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6450784905330359 (run: 7/10).
INFO: Testing took 0:02:07 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:02:07 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.98
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.80
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.55
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.70
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.29
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.74
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.12
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 1.03
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 1.00
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.89
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.63
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.93
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.29
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 1.12
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.10
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 1.73
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.95
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.57
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.75
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.24
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.76
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.08
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 1.27
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.97
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.75
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.57
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.31
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.81
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.16
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.99
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.03
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 1.41
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.91
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.90
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.56
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.84
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.25
  Training epoch took: 0:00:32

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.89
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.08
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.12
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.95
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.90
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.59
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 1.02
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.29
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 1.05
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.10
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 1.78
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.91
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.78
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.50
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.65
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.26
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.72
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.09
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.76
  Validation Loss: 0.83
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.96
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.79
  Validation took: 0:00:01INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:02:06 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.7125781625781626 (run: 8/10).
INFO: Testing took 0:02:07 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 6).
INFO: Training for epoch_poet done.
INFO: Training took 0:03:41 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6105098605098604 (run: 8/10).
INFO: Testing took 0:03:42 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:03:42 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:02:05 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6178210678210678 (run: 9/10).
INFO: Testing took 0:02:06 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:06 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5916546416546417 (run: 9/10).
INFO: Testing took 0:02:07 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:02:07 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:02:06 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.5516835016835017 (run: 10/10).
INFO: Testing took 0:02:07 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:07 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7321067821067822 (run: 10/10).
INFO: Testing took 0:02:08 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:02:08 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 46.75 minute(s).


======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.55
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.68
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.27
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.85
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.11
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.06
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.91
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.66
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.50
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 0.53
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.27
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.67
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.09
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 0.65
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.02
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.81
  Validation Loss: 0.59
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.00
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.81
  Validation Loss: 0.72
  Validation took: 0:00:01

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.00
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.81
  Validation Loss: 0.76
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.95
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.60
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.75
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.27
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.84
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.10
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 1.34
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.90
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.84
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.51
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.80
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.26
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.01
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.10
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.21
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.97
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.43
  Validation Loss: 0.83
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.58
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.76
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.27
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.93
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.07
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 1.38
  Validation took: 0:00:01
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.96
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.44
  Validation Loss: 0.94
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.55
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.75
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:19.

  Average training loss: 0.24
  Training epoch took: 0:00:32

Now Validating.
  Validation Accuracy: 0.49
  Validation Loss: 1.15
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.07
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 1.44
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________

Using TensorFlow backend.
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/tokenization_utils.py:884: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.
  warnings.warn(
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:20 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6622902438691912 (run: 1/10).
INFO: Testing took 0:07:25 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 9).
INFO: Training for epoch_poet done.
INFO: Training took 0:18:39 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7243664717348928 (run: 1/10).
INFO: Testing took 0:18:43 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:18:43 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:16.
Batch   100  of    304.    Elapsed: 0:00:32.
Batch   150  of    304.    Elapsed: 0:00:48.
Batch   200  of    304.    Elapsed: 0:01:05.
Batch   250  of    304.    Elapsed: 0:01:22.
Batch   300  of    304.    Elapsed: 0:01:39.

  Average training loss: 0.72
  Training epoch took: 0:01:40

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.42
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.16
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.90
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.07
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.46
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.82
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.59
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.41
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.20
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.79
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.07
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.76
  Validation Loss: 1.07
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.02
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.79
  Validation Loss: 1.03
  Validation took: 0:00:04

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.01
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.52
  Validation took: 0:00:04

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.00
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 1.42
  Validation took: 0:00:04

======== Epoch 8 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.00
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 1.35
  Validation took: 0:00:04

======== Epoch 9 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.00
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.76
  Validation Loss: 1.38
  Validation took: 0:00:04

======== Epoch 10 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.00
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.76
  Validation Loss: 1.40
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:09.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.71
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.57
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.45
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.21INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_year done.
INFO: Training took 0:09:15 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6720552352131298 (run: 2/10).
INFO: Testing took 0:09:19 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 8).
INFO: Training for epoch_poet done.
INFO: Training took 0:16:43 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7617503354345461 (run: 2/10).
INFO: Testing took 0:16:48 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:16:48 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:30 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6463777450619556 (run: 3/10).
INFO: Testing took 0:07:35 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']

  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.07
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.00
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.01
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.26
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.82
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.66
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.41
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.19
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 1.59
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.09
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.41
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.02
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.89
  Validation took: 0:00:04

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.00
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.81
  Validation took: 0:00:04

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.00
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.66
  Validation took: 0:00:04

======== Epoch 8 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.00
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.72
  Validation took: 0:00:04

======== Epoch 9 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.00
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.82
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.71
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.65
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.42
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.77
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:37.
Batch   150  of    304.    Elapsed: 0:00:55.
Batch   200  of    304.    Elapsed: 0:01:13.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.19
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.13
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.08
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.36
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.84
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:26 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7264485514485516 (run: 3/10).
INFO: Testing took 0:07:31 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:07:31 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:28 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.682724974830238 (run: 4/10).
INFO: Testing took 0:07:33 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:09:26 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.766558977085293 (run: 4/10).
INFO: Testing took 0:09:31 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:09:31 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:31 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6524805116910379 (run: 5/10).
INFO: Testing took 0:07:35 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']

Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.42
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.20
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.78
  Validation Loss: 0.95
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.08
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.76
  Validation Loss: 1.25
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.71
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.60
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.44
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.17
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.15
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.07
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.33
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:13.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.82
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.59
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:19.
Batch   100  of    304.    Elapsed: 0:00:37.
Batch   150  of    304.    Elapsed: 0:00:55.
Batch   200  of    304.    Elapsed: 0:01:13.
Batch   250  of    304.    Elapsed: 0:01:31.
Batch   300  of    304.    Elapsed: 0:01:49.

  Average training loss: 0.43
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 1.00
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:13.
Batch   250  of    304.    Elapsed: 0:01:31.
Batch   300  of    304.    Elapsed: 0:01:49.

  Average training loss: 0.20
  Training epoch took: 0:01:51

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.80
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.06
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.77
  Validation Loss: 0.97
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.03
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.76
  Validation Loss: 1.14
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.70
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:31.
Batch   300  of    304.    Elapsed: 0:01:49.

  Average training loss: 0.39
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.84
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.17
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.00
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.05
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.21
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.81
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.69INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:30 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7152673350041772 (run: 5/10).
INFO: Testing took 0:07:35 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:07:35 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6804727923148975 (run: 6/10).
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:31 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7287182602972077 (run: 6/10).
INFO: Testing took 0:07:36 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:07:36 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6629291274028116 (run: 7/10).
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']

  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.42
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:55.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.17
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.93
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.07
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.76
  Validation Loss: 1.26
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.72
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.61
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.43
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.53
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.20
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.70
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.07
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.11
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.81
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.42
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:13.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:49.

  Average training loss: 0.20
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.99
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.05
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 1.19
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.71
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.49
  Validation Loss: 0.80
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.43
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.91
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.19
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 1.04
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.07
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 1.94
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.82
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.66
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:31 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7329295460874408 (run: 7/10).
INFO: Testing took 0:07:35 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:07:35 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:26 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6594372002266738 (run: 8/10).
INFO: Testing took 0:07:30 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_poet done.
INFO: Training took 0:11:12 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7055694792536898 (run: 8/10).
INFO: Testing took 0:11:17 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:11:17 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']

Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.41
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.81
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.18
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.25
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.08
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.57
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.71
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.43
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.52
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.20
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.85
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.06
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.26
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.81
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.41
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.66
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.20
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 1.34
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.08
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.77
  Validation Loss: 1.07
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.02
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.28
  Validation took: 0:00:04

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.01
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.45
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.72
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.51
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.42
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.60
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.19
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.05
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.07
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.41
  Validation took: 0:00:04INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:26 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6992701255859151 (run: 9/10).
INFO: Testing took 0:07:31 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:32 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6687753766701134 (run: 9/10).
INFO: Testing took 0:07:36 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:07:36 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_year done.
INFO: Training took 0:07:22 (h:mm:ss) 

INFO: Testing for epoch_year done.
INFO: CV Test F1-Score: 0.6724025974025976 (run: 10/10).
INFO: Testing took 0:07:26 (h:mm:ss) 

INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:30 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7118313460418724 (run: 10/10).
INFO: Testing took 0:07:35 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:07:35 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 180.88333333333333 minute(s).

--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.84
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.44
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.17
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 0.96
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.07
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.47
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.70
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.58
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.43
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.52
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.21
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.87
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.08
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.24
  Validation took: 0:00:04
--------------------------------

--------------------------------


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.85
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.44
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.63
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.18
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.99
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.06
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.69
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________

INFO: Starting optimization.
INFO: Argument combination 1/6.
INFO: Batch size: 4.
INFO: Learning rate: 4e-05.
Using TensorFlow backend.
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/tokenization_utils.py:884: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.
  warnings.warn(
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 7).
INFO: Training for epoch_poet done.
INFO: Training took 0:04:40 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.8290764790764791 (run: 1/10).
INFO: Testing took 0:04:41 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:04:41 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 6).
INFO: Training for epoch_poet done.
INFO: Training took 0:04:13 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6598845598845599 (run: 2/10).
INFO: Testing took 0:04:15 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:04:15 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:26 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.523015873015873 (run: 3/10).
INFO: Testing took 0:02:27 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:02:27 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:09.
Batch   100  of    171.    Elapsed: 0:00:19.
Batch   150  of    171.    Elapsed: 0:00:28.

  Average training loss: 0.90
  Training epoch took: 0:00:32

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:19.
Batch   150  of    171.    Elapsed: 0:00:29.

  Average training loss: 0.47
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.93
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:19.
Batch   150  of    171.    Elapsed: 0:00:28.

  Average training loss: 0.29
  Training epoch took: 0:00:32

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 1.54
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:19.
Batch   150  of    171.    Elapsed: 0:00:29.

  Average training loss: 0.07
  Training epoch took: 0:00:33

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 1.42
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.01
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.55
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:11.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:32.

  Average training loss: 0.00
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.27
  Validation took: 0:00:01

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.00
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.35
  Validation took: 0:00:01

======== Epoch 8 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.00
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.41
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.89
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.63
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.48
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 1.38
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.19
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.29
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.05
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 2.30
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.01
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.98
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.00
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 2.17
  Validation took: 0:00:01

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.00
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 2.19
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.92
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.71
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.52
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.75
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.20
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.78
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.04
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 1.87
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.97
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.74
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.60
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.31
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.97
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.08
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.59INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:24 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6235209235209235 (run: 4/10).
INFO: Testing took 0:02:25 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:02:25 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:27 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6489898989898989 (run: 5/10).
INFO: Testing took 0:02:28 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:02:28 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:26 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6967532467532467 (run: 6/10).
INFO: Testing took 0:02:28 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:02:28 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:27 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6064247921390779 (run: 7/10).
INFO: Testing took 0:02:28 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:02:28 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:27 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5656084656084657 (run: 8/10).
INFO: Testing took 0:02:28 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:02:28 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:22 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6778533635676494 (run: 9/10).
INFO: Testing took 0:02:23 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:02:23 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']

  Validation Loss: 1.61
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.96
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.54
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.78
  Validation Loss: 0.58
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.21
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.82
  Validation Loss: 0.78
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:11.
Batch   100  of    171.    Elapsed: 0:00:22.
Batch   150  of    171.    Elapsed: 0:00:32.

  Average training loss: 0.05
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.78
  Validation Loss: 1.05
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.92
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.43
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.52
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.75
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.15
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.27
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.03
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.75
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.89
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.50
  Validation Loss: 0.93
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.50
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.96
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.27
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.29
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:11.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.07
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.48
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.93
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.66
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.50
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.78
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:11.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:32.

  Average training loss: 0.23
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.99
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.05
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.31
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:30.

  Average training loss: 0.89
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.81
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:30.

  Average training loss: 0.54
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:30.

  Average training loss: 0.25
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.07
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:30.

  Average training loss: 0.08
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.22
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.96
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:30.

  Average training loss: 0.51
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:25 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7045351473922904 (run: 10/10).
INFO: Testing took 0:02:27 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:02:27 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 29.183333333333334 minute(s).
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 1.24
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.23
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 1.41
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:30.

  Average training loss: 0.04
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.56
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________

INFO: Argument combination 2/6.
INFO: Batch size: 4.
INFO: Learning rate: 3e-05.
Using TensorFlow backend.
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/tokenization_utils.py:884: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.
  warnings.warn(
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:58 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.8297979797979796 (run: 1/10).
INFO: Testing took 0:03:00 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:03:00 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 6).
INFO: Training for epoch_poet done.
INFO: Training took 0:04:15 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6533189033189033 (run: 2/10).
INFO: Testing took 0:04:16 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:04:16 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:59 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6334054834054834 (run: 3/10).
INFO: Testing took 0:03:01 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:03:01 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:58 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5757575757575758 (run: 4/10).
INFO: Testing took 0:03:00 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:03:00 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.89
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.59
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.46
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.71
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.28
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.79
  Validation Loss: 0.67
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.05
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.79
  Validation Loss: 0.90
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.00
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.76
  Validation Loss: 0.99
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.93
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.68
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.47
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 1.06
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.24
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.76
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.08
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.30
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.02
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.76
  Validation Loss: 1.15
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.00
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.24
  Validation took: 0:00:01

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.00
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 1.36
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.88
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.55
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.49
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 1.06
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.29
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.06
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.78
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.02
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.78
  Validation Loss: 0.98
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.91
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.76
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.46
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.95
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.26
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.89
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.10
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.37
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.00
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 2.51
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:22 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6671717171717171 (run: 5/10).
INFO: Testing took 0:02:24 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:02:24 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:26 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6186868686868686 (run: 6/10).
INFO: Testing took 0:02:27 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:02:27 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:24 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5950113378684808 (run: 7/10).
INFO: Testing took 0:02:25 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:02:25 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 6).
INFO: Training for epoch_poet done.
INFO: Training took 0:04:14 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6444444444444445 (run: 8/10).
INFO: Testing took 0:04:16 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:04:16 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']

Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.88
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 0.93
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.54
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.23
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.48
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.07
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.87
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.89
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.71
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:11.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.41
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.85
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.18
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.36
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.07
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.40
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:30.

  Average training loss: 0.93
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.82
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.47
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.91
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.22
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 1.66
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:30.

  Average training loss: 0.04
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 2.44
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:30.

  Average training loss: 0.90
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.63
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.49
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 1.14
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.17
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.02
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.03
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.55
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:11.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.01
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 1.42
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:30.

  Average training loss: 0.00
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.52
  Validation took: 0:00:01

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:30.

  Average training loss: 0.00
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.53
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:30.

  Average training loss: 0.92
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.50
  Validation Loss: 0.80
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:30.

  Average training loss: 0.47
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 1.24
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:30.

  Average training loss: 0.26
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 1.63
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:11.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:32.

  Average training loss: 0.06
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 1.80
  Validation took: 0:00:01INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:23 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6701436130007559 (run: 9/10).
INFO: Testing took 0:02:25 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:02:25 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:22 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6410430839002268 (run: 10/10).
INFO: Testing took 0:02:24 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:02:24 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 30.283333333333335 minute(s).

--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.89
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:30.

  Average training loss: 0.52
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.86
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:30.

  Average training loss: 0.24
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 1.16
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:30.

  Average training loss: 0.07
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.40
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________

INFO: Argument combination 3/6.
INFO: Batch size: 4.
INFO: Learning rate: 2e-05.
Using TensorFlow backend.
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/tokenization_utils.py:884: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.
  warnings.warn(
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_poet done.
INFO: Training took 0:03:41 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7909090909090909 (run: 1/10).
INFO: Testing took 0:03:43 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:03:43 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:03:04 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6396825396825396 (run: 2/10).
INFO: Testing took 0:03:06 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:03:06 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 7).
INFO: Training for epoch_poet done.
INFO: Training took 0:04:45 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5793650793650793 (run: 3/10).
INFO: Testing took 0:04:47 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:04:47 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.90
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.50
  Validation Loss: 0.98
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.47
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.49
  Validation Loss: 1.31
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:32.

  Average training loss: 0.22
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 1.77
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:11.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:32.

  Average training loss: 0.06
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.76
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:11.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.01
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 1.83
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.00
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 1.91
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:11.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:32.

  Average training loss: 0.90
  Training epoch took: 0:00:37

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 0.81
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:11.
Batch   100  of    171.    Elapsed: 0:00:22.
Batch   150  of    171.    Elapsed: 0:00:33.

  Average training loss: 0.51
  Training epoch took: 0:00:37

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 1.01
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.24
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.74
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.07
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 1.17
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.01
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.40
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.86
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.81
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.40
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.46
  Validation Loss: 1.48
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.13
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 1.97
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.06
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 1.59
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:30.

  Average training loss: 0.01
  Training epoch took: 0:00:34

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 1.93
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.00
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 1.91
  Validation took: 0:00:01

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.00
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 1.92
  Validation took: 0:00:01

======== Epoch 8 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.00
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 1.96
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.86
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.62
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.47
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:11.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.17
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.79
  Validation Loss: 1.09
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.07
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.75INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:26 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5994949494949494 (run: 4/10).
INFO: Testing took 0:02:28 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:02:28 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:26 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5868686868686868 (run: 5/10).
INFO: Testing took 0:02:27 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:02:27 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:33 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6873737373737373 (run: 6/10).
INFO: Testing took 0:02:34 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:02:34 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:26 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7018896447467876 (run: 7/10).
INFO: Testing took 0:02:28 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:02:28 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:25 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.611111111111111 (run: 8/10).
INFO: Testing took 0:02:26 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:02:26 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:28 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5643235071806499 (run: 9/10).
INFO: Testing took 0:02:29 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:02:29 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']

  Validation Loss: 1.21
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:20.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.89
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.69
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.54
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:11.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.20
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 1.23
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:10.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:31.

  Average training loss: 0.05
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 1.45
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:11.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:32.

  Average training loss: 0.90
  Training epoch took: 0:00:37

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:11.
Batch   100  of    171.    Elapsed: 0:00:22.
Batch   150  of    171.    Elapsed: 0:00:33.

  Average training loss: 0.51
  Training epoch took: 0:00:37

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.66
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:11.
Batch   100  of    171.    Elapsed: 0:00:22.
Batch   150  of    171.    Elapsed: 0:00:33.

  Average training loss: 0.23
  Training epoch took: 0:00:37

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.21
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    171.    Elapsed: 0:00:11.
Batch   100  of    171.    Elapsed: 0:00:21.
Batch   150  of    171.    Elapsed: 0:00:32.

  Average training loss: 0.07
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.56
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.89
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.69
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.48
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.89
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:11.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.25
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.07
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.05
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.27
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:30.

  Average training loss: 0.91
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.44
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.89
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.16
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 1.39
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.04
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 1.69
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.89
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.74
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.43
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 1.19
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:11.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.23
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.30
  Validation took: 0:00:02

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:21.
Batch   150  of    172.    Elapsed: 0:00:32.

  Average training loss: 0.05
  Training epoch took: 0:00:36

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.68
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:30.

  Average training loss: 0.90
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.58
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.49
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:24 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.688888888888889 (run: 10/10).
INFO: Testing took 0:02:26 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:02:26 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 29.583333333333332 minute(s).
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.52
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:31.

  Average training loss: 0.21
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.03
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    172.    Elapsed: 0:00:10.
Batch   100  of    172.    Elapsed: 0:00:20.
Batch   150  of    172.    Elapsed: 0:00:30.

  Average training loss: 0.06
  Training epoch took: 0:00:35

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.06
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________

INFO: Argument combination 4/6.
INFO: Batch size: 8.
INFO: Learning rate: 4e-05.
Using TensorFlow backend.
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/tokenization_utils.py:884: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.
  warnings.warn(
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_poet done.
INFO: Training took 0:03:12 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.877080327080327 (run: 1/10).
INFO: Testing took 0:03:13 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:03:13 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:07 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.692377824196006 (run: 2/10).
INFO: Testing took 0:02:09 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:02:09 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:07 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6046176046176046 (run: 3/10).
INFO: Testing took 0:02:08 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:02:08 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:07 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6074161528706983 (run: 4/10).
INFO: Testing took 0:02:09 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:02:09 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:07 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6017575017575018 (run: 5/10).
INFO: Testing took 0:02:08 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:02:08 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:07 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.781890331890332 (run: 6/10).
INFO: Testing took 0:02:08 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:02:08 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.94
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.66
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.46
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 0.61
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.18
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.26
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:19.

  Average training loss: 0.02
  Training epoch took: 0:00:32

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.13
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.00
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 1.15
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.00
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.37
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.91
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 0.85
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.43
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.20
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.42
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.04
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.66
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.90
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 0.92
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.49
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.84
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.16
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 1.58
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.09
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 1.66
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.87
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.41
  Validation Loss: 1.11
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.48
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.62
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.21
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 1.37
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.07
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.62
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.90
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.33
  Validation Loss: 1.41
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.43
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.87
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.21
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 1.41
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.03
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.50
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.88
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.83
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.45
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.18
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.93
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.08
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.42
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.91
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.69
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.51
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.80
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.23
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.77
  Validation Loss: 0.74
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.12INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:38 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6712602212602212 (run: 7/10).
INFO: Testing took 0:02:39 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:02:39 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:05 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5855218855218854 (run: 8/10).
INFO: Testing took 0:02:06 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:02:06 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 7).
INFO: Training for epoch_poet done.
INFO: Training took 0:04:13 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6566378066378066 (run: 9/10).
INFO: Testing took 0:04:14 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:04:14 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 6).
INFO: Training for epoch_poet done.
INFO: Training took 0:03:43 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7697931697931698 (run: 10/10).
INFO: Testing took 0:03:45 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:03:45 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 27.333333333333332 minute(s).

  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.79
  Validation Loss: 0.97
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.01
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.77
  Validation Loss: 1.11
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.87
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.65
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.47
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.64
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.17
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.90
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.10
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 1.51
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.89
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.64
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.39
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.60
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.15
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.20
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.04
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.77
  Validation Loss: 0.81
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.00
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.58
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.00
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.79
  Validation Loss: 0.92
  Validation took: 0:00:01

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.00
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.19
  Validation took: 0:00:01

======== Epoch 8 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.00
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.25
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.88
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.48
  Validation Loss: 0.96
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.40
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.97
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.15
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 1.63
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.07
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.26
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.02
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 1.11
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.01
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.32
  Validation took: 0:00:01

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.00
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.44
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________

INFO: Argument combination 5/6.
INFO: Batch size: 8.
INFO: Learning rate: 3e-05.
Using TensorFlow backend.
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/tokenization_utils.py:884: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.
  warnings.warn(
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 7).
INFO: Training for epoch_poet done.
INFO: Training took 0:04:14 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.8993746993746993 (run: 1/10).
INFO: Testing took 0:04:15 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:04:15 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:05 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6539460539460539 (run: 2/10).
INFO: Testing took 0:02:07 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:02:07 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:39 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6763347763347763 (run: 3/10).
INFO: Testing took 0:02:41 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:02:41 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:07 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6554965236783418 (run: 4/10).
INFO: Testing took 0:02:08 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:02:08 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:37 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6333814333814334 (run: 5/10).
INFO: Testing took 0:02:39 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:02:39 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.95
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.79
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.47
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.67
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.22
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 1.14
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.06
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 1.14
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.01
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.15
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.00
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.13
  Validation took: 0:00:01

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.00
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 1.31
  Validation took: 0:00:01

======== Epoch 8 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.00
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.39
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.91
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.70
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.45
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.65
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.15
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.03
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.03
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.13
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.90
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.73
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.47
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.20
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.85
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.06
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.04
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.01
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.19
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.94
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.76
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.54
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.25
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.06
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.42
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.89
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.77
  Validation Loss: 0.61
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.46
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.66
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.23
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.82
  Validation Loss: 0.56
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.06
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.78
  Validation Loss: 0.88
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.01
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.82
  Validation Loss: 0.89
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.88
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.65
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.41
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.67
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.18
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.94
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.06
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.13
  Validation took: 0:00:01INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:07 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7094516594516593 (run: 6/10).
INFO: Testing took 0:02:09 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:02:09 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_poet done.
INFO: Training took 0:03:08 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6976911976911977 (run: 7/10).
INFO: Testing took 0:03:09 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:03:09 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:06 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6311207311207312 (run: 8/10).
INFO: Testing took 0:02:07 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:02:07 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:07 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5537518037518038 (run: 9/10).
INFO: Testing took 0:02:08 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:02:08 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:07 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6470702697975426 (run: 10/10).
INFO: Testing took 0:02:09 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:02:09 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 26.25 minute(s).

--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.90
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.70
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.46
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.78
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.17
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.07
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.05
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.01
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.00
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.14
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.00
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.65
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.88
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.78
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.46
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.63
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.15
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.81
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.07
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.41
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.93
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.45
  Validation Loss: 0.89
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.46
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.16
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.97
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.07
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 1.34
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.92
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.50
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.48
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.74
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.24
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.91
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.09
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 1.35
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________

INFO: Argument combination 6/6.
INFO: Batch size: 8.
INFO: Learning rate: 2e-05.
Using TensorFlow backend.
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/tokenization_utils.py:884: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.
  warnings.warn(
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:06 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.8781363417727054 (run: 1/10).
INFO: Testing took 0:02:07 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:02:07 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:07 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6615440115440115 (run: 2/10).
INFO: Testing took 0:02:09 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:02:09 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_poet done.
INFO: Training took 0:03:14 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6517556517556516 (run: 3/10).
INFO: Testing took 0:03:15 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:03:15 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:08 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6425816607634789 (run: 4/10).
INFO: Testing took 0:02:09 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:02:09 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:39 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6017075517075517 (run: 5/10).
INFO: Testing took 0:02:40 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:02:40 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:06 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7236040054221872 (run: 6/10).
INFO: Testing took 0:02:07 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:02:07 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.95
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.78
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.52
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.82
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.26
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 1.01
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.10
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.31
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.95
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 0.89
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.53
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.49
  Validation Loss: 0.85
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.22
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 0.91
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.06
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 1.51
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.92
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.74
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.52
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.73
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:19.

  Average training loss: 0.24
  Training epoch took: 0:00:32

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 1.23
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.09
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.17
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.03
  Training epoch took: 0:00:32

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.33
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:19.

  Average training loss: 0.01
  Training epoch took: 0:00:32

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.36
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.99
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.46
  Validation Loss: 0.89
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:19.

  Average training loss: 0.54
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.84
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.23
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 1.09
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.09
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 1.46
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.93
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.81
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.54
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.89
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.24
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.83
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.08
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.41
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.04
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.66
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.94
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.47
  Validation Loss: 0.76
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.49
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.80
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.20
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.00
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.06
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 1.38
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.92
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.77
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.52
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.70
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.21INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:05 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7517464017464017 (run: 7/10).
INFO: Testing took 0:02:06 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:02:06 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_poet done.
INFO: Training took 0:03:09 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5864117364117363 (run: 8/10).
INFO: Testing took 0:03:11 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:03:11 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:06 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.5800144300144301 (run: 9/10).
INFO: Testing took 0:02:08 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:02:08 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:02:40 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7408653635926363 (run: 10/10).
INFO: Testing took 0:02:41 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:02:41 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 25.283333333333335 minute(s).

  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.77
  Validation Loss: 0.73
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.10
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.05
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.96
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.72
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.56
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.60
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.28
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.93
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.12
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.85
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.03
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.33
  Validation took: 0:00:01

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.01
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.51
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:17.

  Average training loss: 0.94
  Training epoch took: 0:00:30

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.65
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.52
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.56
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.28
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.77
  Validation Loss: 0.65
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.09
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.95
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.94
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.71
  Validation took: 0:00:01

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.54
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.58
  Validation took: 0:00:01

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.28
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.57
  Validation took: 0:00:01

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.11
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.08
  Validation took: 0:00:01

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of     86.    Elapsed: 0:00:18.

  Average training loss: 0.04
  Training epoch took: 0:00:31

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.15
  Validation took: 0:00:01
--------------------------------

--------------------------------

________________________________
________________________________

INFO: Overall run-time: 168.25 minute(s).
--------------------------------------------
--------------------------------------------


--------------------------------------------
--------------------------------------------


--------------------------------------------
--------------------------------------------


--------------------------------------------
--------------------------------------------


--------------------------------------------
--------------------------------------------


--------------------------------------------
--------------------------------------------


INFO: Starting optimization.
INFO: Argument combination 1/6.
INFO: Batch size: 4.
INFO: Learning rate: 4e-05.
Using TensorFlow backend.
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/tokenization_utils.py:884: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.
  warnings.warn(
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:22 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.674031746031746 (run: 1/10).
INFO: Testing took 0:08:27 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:08:27 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:32 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.693925925925926 (run: 2/10).
INFO: Testing took 0:08:38 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:08:38 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:09.
Batch   100  of    608.    Elapsed: 0:00:19.
Batch   150  of    608.    Elapsed: 0:00:28.
Batch   200  of    608.    Elapsed: 0:00:38.
Batch   250  of    608.    Elapsed: 0:00:47.
Batch   300  of    608.    Elapsed: 0:00:57.
Batch   350  of    608.    Elapsed: 0:01:06.
Batch   400  of    608.    Elapsed: 0:01:16.
Batch   450  of    608.    Elapsed: 0:01:26.
Batch   500  of    608.    Elapsed: 0:01:36.
Batch   550  of    608.    Elapsed: 0:01:46.
Batch   600  of    608.    Elapsed: 0:01:56.

  Average training loss: 0.89
  Training epoch took: 0:01:57

Now Validating.
  Validation Accuracy: 0.42
  Validation Loss: 1.42
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.59
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.80
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:10.
Batch   400  of    608.    Elapsed: 0:01:20.
Batch   450  of    608.    Elapsed: 0:01:30.
Batch   500  of    608.    Elapsed: 0:01:40.
Batch   550  of    608.    Elapsed: 0:01:50.
Batch   600  of    608.    Elapsed: 0:02:00.

  Average training loss: 0.30
  Training epoch took: 0:02:02

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.14
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:30.
Batch   500  of    608.    Elapsed: 0:01:40.
Batch   550  of    608.    Elapsed: 0:01:50.
Batch   600  of    608.    Elapsed: 0:02:00.

  Average training loss: 0.16
  Training epoch took: 0:02:02

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 1.65
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:19.
Batch   150  of    608.    Elapsed: 0:00:29.
Batch   200  of    608.    Elapsed: 0:00:39.
Batch   250  of    608.    Elapsed: 0:00:49.
Batch   300  of    608.    Elapsed: 0:00:59.
Batch   350  of    608.    Elapsed: 0:01:09.
Batch   400  of    608.    Elapsed: 0:01:19.
Batch   450  of    608.    Elapsed: 0:01:29.
Batch   500  of    608.    Elapsed: 0:01:39.
Batch   550  of    608.    Elapsed: 0:01:50.
Batch   600  of    608.    Elapsed: 0:02:00.

  Average training loss: 0.85
  Training epoch took: 0:02:01

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.90
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:04.

  Average training loss: 0.53
  Training epoch took: 0:02:06

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 1.20
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.26
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.23
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.09
  Training epoch took: 0:02:03

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.71
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:04.

  Average training loss: 0.85
  Training epoch took: 0:02:06

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.95
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:13.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:04.

  Average training loss: 0.53
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.59
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:37 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6509417989417989 (run: 3/10).
INFO: Testing took 0:08:42 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:08:42 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:10:34 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7513015873015874 (run: 4/10).
INFO: Testing took 0:10:39 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:10:39 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:35 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7334391534391534 (run: 5/10).
INFO: Testing took 0:08:40 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:08:40 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']

Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.27
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.76
  Validation Loss: 0.77
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.07
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 1.29
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:19.
Batch   150  of    608.    Elapsed: 0:00:29.
Batch   200  of    608.    Elapsed: 0:00:39.
Batch   250  of    608.    Elapsed: 0:00:49.
Batch   300  of    608.    Elapsed: 0:00:59.
Batch   350  of    608.    Elapsed: 0:01:09.
Batch   400  of    608.    Elapsed: 0:01:19.
Batch   450  of    608.    Elapsed: 0:01:29.
Batch   500  of    608.    Elapsed: 0:01:39.
Batch   550  of    608.    Elapsed: 0:01:49.
Batch   600  of    608.    Elapsed: 0:01:59.

  Average training loss: 0.88
  Training epoch took: 0:02:01

Now Validating.
  Validation Accuracy: 0.54
  Validation Loss: 0.85
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:10.
Batch   400  of    608.    Elapsed: 0:01:20.
Batch   450  of    608.    Elapsed: 0:01:30.
Batch   500  of    608.    Elapsed: 0:01:40.
Batch   550  of    608.    Elapsed: 0:01:50.
Batch   600  of    608.    Elapsed: 0:02:00.

  Average training loss: 0.57
  Training epoch took: 0:02:01

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 1.36
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:10.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:51.
Batch   600  of    608.    Elapsed: 0:02:01.

  Average training loss: 0.23
  Training epoch took: 0:02:03

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.09
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.11
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.29
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.03
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.92
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.87
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.60
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:42.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:03.
Batch   350  of    608.    Elapsed: 0:01:13.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:34.
Batch   500  of    608.    Elapsed: 0:01:44.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:05.

  Average training loss: 0.58
  Training epoch took: 0:02:06

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.83
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:11.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:51.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.28
  Training epoch took: 0:02:03

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.07
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.09
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.21
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:29 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6845925925925926 (run: 6/10).
INFO: Testing took 0:08:34 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:08:34 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:29 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6647619047619048 (run: 7/10).
INFO: Testing took 0:08:34 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:08:34 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']

Batch   100  of    608.    Elapsed: 0:00:19.
Batch   150  of    608.    Elapsed: 0:00:29.
Batch   200  of    608.    Elapsed: 0:00:39.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:10.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.87
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.74
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:10.
Batch   400  of    608.    Elapsed: 0:01:20.
Batch   450  of    608.    Elapsed: 0:01:30.
Batch   500  of    608.    Elapsed: 0:01:40.
Batch   550  of    608.    Elapsed: 0:01:50.
Batch   600  of    608.    Elapsed: 0:02:00.

  Average training loss: 0.54
  Training epoch took: 0:02:01

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.14
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:40.
Batch   550  of    608.    Elapsed: 0:01:51.
Batch   600  of    608.    Elapsed: 0:02:01.

  Average training loss: 0.23
  Training epoch took: 0:02:02

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.23
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.07
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.71
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:10.
Batch   400  of    608.    Elapsed: 0:01:20.
Batch   450  of    608.    Elapsed: 0:01:30.
Batch   500  of    608.    Elapsed: 0:01:40.
Batch   550  of    608.    Elapsed: 0:01:50.
Batch   600  of    608.    Elapsed: 0:02:00.

  Average training loss: 0.85
  Training epoch took: 0:02:02

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.49
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.80
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.23
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.10
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:10.
Batch   400  of    608.    Elapsed: 0:01:20.
Batch   450  of    608.    Elapsed: 0:01:29.
Batch   500  of    608.    Elapsed: 0:01:40.
Batch   550  of    608.    Elapsed: 0:01:50.
Batch   600  of    608.    Elapsed: 0:02:00.

  Average training loss: 0.07
  Training epoch took: 0:02:02

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.50
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.85
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.74
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:51.
Batch   600  of    608.    Elapsed: 0:02:01.

  Average training loss: 0.52
  Training epoch took: 0:02:03

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.88
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:49.
Batch   300  of    608.    Elapsed: 0:00:59.
Batch   350  of    608.    Elapsed: 0:01:09.
Batch   400  of    608.    Elapsed: 0:01:19.
Batch   450  of    608.    Elapsed: 0:01:30.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:51.
Batch   600  of    608.    Elapsed: 0:02:02.INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_poet done.
INFO: Training took 0:12:48 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6577142857142857 (run: 8/10).
INFO: Testing took 0:12:53 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:12:53 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:27 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6550687830687831 (run: 9/10).
INFO: Testing took 0:08:31 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:08:31 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']


  Average training loss: 0.21
  Training epoch took: 0:02:03

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.55
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.10
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.25
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.03
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.43
  Validation took: 0:00:04

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.00
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.69
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:51.
Batch   600  of    608.    Elapsed: 0:02:01.

  Average training loss: 0.85
  Training epoch took: 0:02:03

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.63
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:10.
Batch   400  of    608.    Elapsed: 0:01:20.
Batch   450  of    608.    Elapsed: 0:01:30.
Batch   500  of    608.    Elapsed: 0:01:40.
Batch   550  of    608.    Elapsed: 0:01:50.
Batch   600  of    608.    Elapsed: 0:02:00.

  Average training loss: 0.50
  Training epoch took: 0:02:01

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.09
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:10.
Batch   400  of    608.    Elapsed: 0:01:20.
Batch   450  of    608.    Elapsed: 0:01:30.
Batch   500  of    608.    Elapsed: 0:01:40.
Batch   550  of    608.    Elapsed: 0:01:50.
Batch   600  of    608.    Elapsed: 0:02:01.

  Average training loss: 0.20
  Training epoch took: 0:02:02

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.26
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.07
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.39
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.83
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.73
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:13.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:34.
Batch   500  of    608.    Elapsed: 0:01:44.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:05.

  Average training loss: 0.50
  Training epoch took: 0:02:06

Now Validating.
  Validation Accuracy: 0.52
  Validation Loss: 1.70
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:11.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:44.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:04.

  Average training loss: 0.20
  Training epoch took: 0:02:06

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.54
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:42.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:10:45 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7242962962962962 (run: 10/10).
INFO: Testing took 0:10:50 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:10:50 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 95.6 minute(s).

Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.09
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.79
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.04
  Training epoch took: 0:02:03

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.99
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________

INFO: Argument combination 2/6.
INFO: Batch size: 4.
INFO: Learning rate: 3e-05.
Using TensorFlow backend.
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/tokenization_utils.py:884: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.
  warnings.warn(
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:26 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.683068783068783 (run: 1/10).
INFO: Testing took 0:08:31 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:08:31 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:32 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7220952380952381 (run: 2/10).
INFO: Testing took 0:08:37 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:08:37 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:13.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:44.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:05.

  Average training loss: 0.82
  Training epoch took: 0:02:06

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.93
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:11.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:51.
Batch   600  of    608.    Elapsed: 0:02:01.

  Average training loss: 0.51
  Training epoch took: 0:02:02

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.25
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:10.
Batch   400  of    608.    Elapsed: 0:01:20.
Batch   450  of    608.    Elapsed: 0:01:30.
Batch   500  of    608.    Elapsed: 0:01:39.
Batch   550  of    608.    Elapsed: 0:01:49.
Batch   600  of    608.    Elapsed: 0:01:59.

  Average training loss: 0.24
  Training epoch took: 0:02:01

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.33
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:09.
Batch   400  of    608.    Elapsed: 0:01:19.
Batch   450  of    608.    Elapsed: 0:01:29.
Batch   500  of    608.    Elapsed: 0:01:39.
Batch   550  of    608.    Elapsed: 0:01:49.
Batch   600  of    608.    Elapsed: 0:01:59.

  Average training loss: 0.09
  Training epoch took: 0:02:00

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 2.03
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.85
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.78
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.51
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 1.22
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:10.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.24
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 1.76
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:11.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:32.
Batch   200  of    608.    Elapsed: 0:00:42.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:51.
Batch   600  of    608.    Elapsed: 0:02:01.

  Average training loss: 0.07
  Training epoch took: 0:02:03

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 1.96
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:04.

  Average training loss: 0.84
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:11.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:42.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:13.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:34.
Batch   500  of    608.    Elapsed: 0:01:44.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:04.

  Average training loss: 0.46
  Training epoch took: 0:02:06

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 1.51
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:10.
Batch   400  of    608.    Elapsed: 0:01:20.
Batch   450  of    608.    Elapsed: 0:01:30.INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:10:37 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7586243386243385 (run: 3/10).
INFO: Testing took 0:10:42 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:10:42 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:33 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6873650793650794 (run: 4/10).
INFO: Testing took 0:08:38 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:08:38 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:33 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.735936507936508 (run: 5/10).
INFO: Testing took 0:08:39 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:08:39 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']

Batch   500  of    608.    Elapsed: 0:01:40.
Batch   550  of    608.    Elapsed: 0:01:49.
Batch   600  of    608.    Elapsed: 0:01:59.

  Average training loss: 0.18
  Training epoch took: 0:02:01

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 1.04
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.07
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.48
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:10.
Batch   400  of    608.    Elapsed: 0:01:19.
Batch   450  of    608.    Elapsed: 0:01:29.
Batch   500  of    608.    Elapsed: 0:01:39.
Batch   550  of    608.    Elapsed: 0:01:49.
Batch   600  of    608.    Elapsed: 0:01:59.

  Average training loss: 0.02
  Training epoch took: 0:02:01

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 1.64
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.84
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.65
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:10.
Batch   400  of    608.    Elapsed: 0:01:20.
Batch   450  of    608.    Elapsed: 0:01:30.
Batch   500  of    608.    Elapsed: 0:01:40.
Batch   550  of    608.    Elapsed: 0:01:51.
Batch   600  of    608.    Elapsed: 0:02:01.

  Average training loss: 0.54
  Training epoch took: 0:02:02

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.17
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:04.

  Average training loss: 0.21
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 1.19
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.06
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.53
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:10.
Batch   400  of    608.    Elapsed: 0:01:20.
Batch   450  of    608.    Elapsed: 0:01:30.
Batch   500  of    608.    Elapsed: 0:01:40.
Batch   550  of    608.    Elapsed: 0:01:50.
Batch   600  of    608.    Elapsed: 0:02:00.

  Average training loss: 0.83
  Training epoch took: 0:02:02

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.81
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.55
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 1.17
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:04.

  Average training loss: 0.25
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.35
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.11
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.50
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:34 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6512592592592593 (run: 6/10).
INFO: Testing took 0:08:39 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:08:39 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:38 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7119999999999999 (run: 7/10).
INFO: Testing took 0:08:43 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:08:43 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']

Batch   100  of    608.    Elapsed: 0:00:19.
Batch   150  of    608.    Elapsed: 0:00:29.
Batch   200  of    608.    Elapsed: 0:00:39.
Batch   250  of    608.    Elapsed: 0:00:49.
Batch   300  of    608.    Elapsed: 0:00:59.
Batch   350  of    608.    Elapsed: 0:01:09.
Batch   400  of    608.    Elapsed: 0:01:19.
Batch   450  of    608.    Elapsed: 0:01:29.
Batch   500  of    608.    Elapsed: 0:01:40.
Batch   550  of    608.    Elapsed: 0:01:50.
Batch   600  of    608.    Elapsed: 0:02:00.

  Average training loss: 0.83
  Training epoch took: 0:02:01

Now Validating.
  Validation Accuracy: 0.57
  Validation Loss: 0.79
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:44.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:04.

  Average training loss: 0.46
  Training epoch took: 0:02:06

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.07
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:13.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:34.
Batch   500  of    608.    Elapsed: 0:01:44.
Batch   550  of    608.    Elapsed: 0:01:55.
Batch   600  of    608.    Elapsed: 0:02:05.

  Average training loss: 0.18
  Training epoch took: 0:02:07

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 1.85
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:11.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:42.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:13.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.06
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 2.10
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.84
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:44.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:05.

  Average training loss: 0.49
  Training epoch took: 0:02:06

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.06
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:44.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:04.

  Average training loss: 0.23
  Training epoch took: 0:02:06

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.13
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:13.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:04.

  Average training loss: 0.05
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 1.46
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:04.

  Average training loss: 0.85
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.65
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:13.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:34.
Batch   500  of    608.    Elapsed: 0:01:44.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:05.

  Average training loss: 0.53
  Training epoch took: 0:02:06

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.85
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:42.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:04.INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:39 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6483809523809524 (run: 8/10).
INFO: Testing took 0:08:44 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:08:44 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:35 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6911957671957671 (run: 9/10).
INFO: Testing took 0:08:40 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:08:40 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:37 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7087407407407407 (run: 10/10).
INFO: Testing took 0:08:42 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:08:42 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 89.71666666666667 minute(s).


  Average training loss: 0.21
  Training epoch took: 0:02:06

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.23
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:04.

  Average training loss: 0.06
  Training epoch took: 0:02:06

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.78
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:04.

  Average training loss: 0.84
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 0.74
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:42.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:13.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:04.

  Average training loss: 0.51
  Training epoch took: 0:02:06

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.16
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:51.
Batch   600  of    608.    Elapsed: 0:02:01.

  Average training loss: 0.23
  Training epoch took: 0:02:03

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 1.39
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.10
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.43
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:29.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.83
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.92
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:13.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:34.
Batch   500  of    608.    Elapsed: 0:01:44.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:05.

  Average training loss: 0.47
  Training epoch took: 0:02:06

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 1.19
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.21
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.28
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:04.

  Average training loss: 0.11
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.46
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________

INFO: Argument combination 3/6.
INFO: Batch size: 4.
INFO: Learning rate: 2e-05.
Using TensorFlow backend.
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/tokenization_utils.py:884: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.
  warnings.warn(
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:34 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7273439153439153 (run: 1/10).
INFO: Testing took 0:08:39 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:08:39 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:34 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7076825396825398 (run: 2/10).
INFO: Testing took 0:08:39 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:08:39 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:44.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:04.

  Average training loss: 0.82
  Training epoch took: 0:02:06

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.73
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:42.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.51
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.93
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.22
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.41
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:51.
Batch   600  of    608.    Elapsed: 0:02:01.

  Average training loss: 0.08
  Training epoch took: 0:02:02

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.44
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.84
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.65
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:13.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:44.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:04.

  Average training loss: 0.50
  Training epoch took: 0:02:06

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.91
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.24
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.53
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:50.
Batch   600  of    608.    Elapsed: 0:02:00.

  Average training loss: 0.07
  Training epoch took: 0:02:02

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 1.95
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:19.
Batch   150  of    608.    Elapsed: 0:00:29.
Batch   200  of    608.    Elapsed: 0:00:39.
Batch   250  of    608.    Elapsed: 0:00:49.
Batch   300  of    608.    Elapsed: 0:00:59.
Batch   350  of    608.    Elapsed: 0:01:09.
Batch   400  of    608.    Elapsed: 0:01:19.
Batch   450  of    608.    Elapsed: 0:01:29.
Batch   500  of    608.    Elapsed: 0:01:39.
Batch   550  of    608.    Elapsed: 0:01:49.
Batch   600  of    608.    Elapsed: 0:01:59.

  Average training loss: 0.82
  Training epoch took: 0:02:01

Now Validating.
  Validation Accuracy: 0.53
  Validation Loss: 0.86
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:11.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:51.
Batch   600  of    608.    Elapsed: 0:02:01.

  Average training loss: 0.44
  Training epoch took: 0:02:03

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.88
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:10.
Batch   400  of    608.    Elapsed: 0:01:20.
Batch   450  of    608.    Elapsed: 0:01:30.INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:23 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6824761904761906 (run: 3/10).
INFO: Testing took 0:08:28 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:08:28 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:28 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7385396825396826 (run: 4/10).
INFO: Testing took 0:08:33 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:08:33 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:33 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7222222222222223 (run: 5/10).
INFO: Testing took 0:08:38 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:08:38 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']

Batch   500  of    608.    Elapsed: 0:01:40.
Batch   550  of    608.    Elapsed: 0:01:50.
Batch   600  of    608.    Elapsed: 0:02:00.

  Average training loss: 0.19
  Training epoch took: 0:02:02

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.19
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:20.
Batch   450  of    608.    Elapsed: 0:01:30.
Batch   500  of    608.    Elapsed: 0:01:40.
Batch   550  of    608.    Elapsed: 0:01:50.
Batch   600  of    608.    Elapsed: 0:02:00.

  Average training loss: 0.06
  Training epoch took: 0:02:01

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.64
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:10.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.82
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.63
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:51.
Batch   600  of    608.    Elapsed: 0:02:01.

  Average training loss: 0.48
  Training epoch took: 0:02:03

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.91
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.20
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.07
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:10.
Batch   400  of    608.    Elapsed: 0:01:20.
Batch   450  of    608.    Elapsed: 0:01:30.
Batch   500  of    608.    Elapsed: 0:01:40.
Batch   550  of    608.    Elapsed: 0:01:50.
Batch   600  of    608.    Elapsed: 0:02:00.

  Average training loss: 0.04
  Training epoch took: 0:02:01

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.28
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.83
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.52
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:13.
Batch   400  of    608.    Elapsed: 0:01:23.
Batch   450  of    608.    Elapsed: 0:01:34.
Batch   500  of    608.    Elapsed: 0:01:44.
Batch   550  of    608.    Elapsed: 0:01:54.
Batch   600  of    608.    Elapsed: 0:02:05.

  Average training loss: 0.47
  Training epoch took: 0:02:06

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.89
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:50.
Batch   600  of    608.    Elapsed: 0:02:00.

  Average training loss: 0.19
  Training epoch took: 0:02:02

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.10
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:10.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:51.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.07
  Training epoch took: 0:02:03

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 1.29
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.83
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.63
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:34 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7149629629629629 (run: 6/10).
INFO: Testing took 0:08:40 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:08:40 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:28 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7053333333333334 (run: 7/10).
INFO: Testing took 0:08:33 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:08:33 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']

Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:04.

  Average training loss: 0.46
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.76
  Validation Loss: 0.87
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:11.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:04.

  Average training loss: 0.19
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 1.04
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:51.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.07
  Training epoch took: 0:02:03

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 1.31
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:51.
Batch   600  of    608.    Elapsed: 0:02:01.

  Average training loss: 0.82
  Training epoch took: 0:02:02

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.79
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:10.
Batch   400  of    608.    Elapsed: 0:01:20.
Batch   450  of    608.    Elapsed: 0:01:30.
Batch   500  of    608.    Elapsed: 0:01:40.
Batch   550  of    608.    Elapsed: 0:01:50.
Batch   600  of    608.    Elapsed: 0:02:00.

  Average training loss: 0.46
  Training epoch took: 0:02:01

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.00
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.17
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.64
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.08
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.92
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.83
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.59
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.44
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.08
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.18
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.53
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_poet done.
INFO: Training took 0:12:49 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6734179894179894 (run: 8/10).
INFO: Testing took 0:12:54 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:12:54 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:31 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6902857142857143 (run: 9/10).
INFO: Testing took 0:08:36 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:08:36 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:08:31 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7294814814814814 (run: 10/10).
INFO: Testing took 0:08:36 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:08:36 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 91.31666666666666 minute(s).


  Average training loss: 0.06
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.47
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:51.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.03
  Training epoch took: 0:02:03

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.58
  Validation took: 0:00:04

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:33.
Batch   500  of    608.    Elapsed: 0:01:43.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.02
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.71
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.84
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.74
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:52.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.49
  Training epoch took: 0:02:03

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.76
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:50.
Batch   300  of    608.    Elapsed: 0:01:00.
Batch   350  of    608.    Elapsed: 0:01:10.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:51.
Batch   600  of    608.    Elapsed: 0:02:01.

  Average training loss: 0.24
  Training epoch took: 0:02:03

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.94
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.06
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.13
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:30.
Batch   200  of    608.    Elapsed: 0:00:40.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.83
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:01.
Batch   350  of    608.    Elapsed: 0:01:11.
Batch   400  of    608.    Elapsed: 0:01:21.
Batch   450  of    608.    Elapsed: 0:01:31.
Batch   500  of    608.    Elapsed: 0:01:41.
Batch   550  of    608.    Elapsed: 0:01:50.
Batch   600  of    608.    Elapsed: 0:02:01.

  Average training loss: 0.48
  Training epoch took: 0:02:02

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.82
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:20.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:52.
Batch   600  of    608.    Elapsed: 0:02:02.

  Average training loss: 0.18
  Training epoch took: 0:02:04

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.40
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    608.    Elapsed: 0:00:10.
Batch   100  of    608.    Elapsed: 0:00:21.
Batch   150  of    608.    Elapsed: 0:00:31.
Batch   200  of    608.    Elapsed: 0:00:41.
Batch   250  of    608.    Elapsed: 0:00:51.
Batch   300  of    608.    Elapsed: 0:01:02.
Batch   350  of    608.    Elapsed: 0:01:12.
Batch   400  of    608.    Elapsed: 0:01:22.
Batch   450  of    608.    Elapsed: 0:01:32.
Batch   500  of    608.    Elapsed: 0:01:42.
Batch   550  of    608.    Elapsed: 0:01:53.
Batch   600  of    608.    Elapsed: 0:02:03.

  Average training loss: 0.07
  Training epoch took: 0:02:05

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.53
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________

INFO: Argument combination 4/6.
INFO: Batch size: 8.
INFO: Learning rate: 4e-05.
Using TensorFlow backend.
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/tokenization_utils.py:884: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.
  warnings.warn(
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_poet done.
INFO: Training took 0:11:16 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7368629908103592 (run: 1/10).
INFO: Testing took 0:11:21 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:11:21 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.767224880382775 (run: 2/10).
INFO: Testing took 0:07:33 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:07:33 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_poet done.
INFO: Training took 0:11:04 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.743051149630097 (run: 3/10).
INFO: Testing took 0:11:09 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:11:09 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.82
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:13.
Batch   250  of    304.    Elapsed: 0:01:31.
Batch   300  of    304.    Elapsed: 0:01:49.

  Average training loss: 0.41
  Training epoch took: 0:01:51

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.61
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.20
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.26
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.06
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 1.03
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.02
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 1.34
  Validation took: 0:00:04

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.00
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.76
  Validation Loss: 1.34
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.81
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:49.

  Average training loss: 0.41
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.83
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:37.
Batch   150  of    304.    Elapsed: 0:00:55.
Batch   200  of    304.    Elapsed: 0:01:13.
Batch   250  of    304.    Elapsed: 0:01:31.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.16
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.33
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.07
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.80
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.83
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.59
  Validation Loss: 0.73
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.42
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 1.05
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.21
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 1.69
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.08
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.58
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.02
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.89
  Validation took: 0:00:04

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.01
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 2.10
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:21 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7419874472506053 (run: 4/10).
INFO: Testing took 0:07:26 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:07:26 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:20 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7458399787347156 (run: 5/10).
INFO: Testing took 0:07:24 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:07:24 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_poet done.
INFO: Training took 0:11:09 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6871832358674466 (run: 6/10).
INFO: Testing took 0:11:14 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:11:14 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']

Batch   200  of    304.    Elapsed: 0:01:09.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.85
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.56
  Validation Loss: 0.89
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.46
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.91
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.23
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 1.00
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.09
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.49
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:09.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.81
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.65
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.40
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.74
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.17
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 1.09
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.06
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.52
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.83
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.51
  Validation Loss: 1.10
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.43
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.56
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.18
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.16
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.07
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 1.08
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.02
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.76
  Validation Loss: 1.34
  Validation took: 0:00:04

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.01
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.71
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.79
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.39
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.90
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.16
  Training epoch took: 0:01:47

Now Validating.INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_poet done.
INFO: Training took 0:11:06 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7226430976430978 (run: 7/10).
INFO: Testing took 0:11:10 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:11:10 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:27 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6889686337054758 (run: 8/10).
INFO: Testing took 0:07:31 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:07:31 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']

  Validation Accuracy: 0.73
  Validation Loss: 1.23
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.07
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.77
  Validation Loss: 1.21
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.02
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 1.44
  Validation took: 0:00:04

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.01
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 1.44
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.80
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.58
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.37
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.13
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.79
  Validation Loss: 1.00
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.05
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 1.16
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.82
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.39
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.58
  Validation Loss: 1.33
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.20
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.07
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.07
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.79
  Validation Loss: 1.03
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.02
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.81
  Validation Loss: 1.09
  Validation took: 0:00:04

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.02
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.80
  Validation Loss: 1.08
  Validation took: 0:00:04

======== Epoch 7 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:44.

  Average training loss: 0.00
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.79
  Validation Loss: 0.99
  Validation took: 0:00:04

======== Epoch 8 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:44.

  Average training loss: 0.00
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.79
  Validation Loss: 1.11
  Validation took: 0:00:04

======== Epoch 9 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.00
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.79
  Validation Loss: 1.06
  Validation took: 0:00:04

======== Epoch 10 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.INFO: Training for epoch_poet done.
INFO: Training took 0:18:26 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7164608296187243 (run: 9/10).
INFO: Testing took 0:18:31 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:18:31 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:23 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7024638616743881 (run: 10/10).
INFO: Testing took 0:07:27 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:07:27 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 101.81666666666666 minute(s).

Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.00
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.80
  Validation Loss: 1.04
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:09.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.81
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.66
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.40
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.96
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.17
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.37
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.07
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.62
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________

INFO: Argument combination 5/6.
INFO: Batch size: 8.
INFO: Learning rate: 3e-05.
Using TensorFlow backend.
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/tokenization_utils.py:884: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.
  warnings.warn(
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:30 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7231551099972153 (run: 1/10).
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:07:34 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:31 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7700254423938634 (run: 2/10).
INFO: Testing took 0:07:36 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:07:36 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7129216787111524 (run: 3/10).
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:07:34 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:26 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7521865561339246 (run: 4/10).
INFO: Testing took 0:07:31 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:07:31 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.82
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.42
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.70
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.19
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.02
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.07
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.20
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.78
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.61
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:13.
Batch   250  of    304.    Elapsed: 0:01:31.
Batch   300  of    304.    Elapsed: 0:01:49.

  Average training loss: 0.38
  Training epoch took: 0:01:51

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 0.58
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.18
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.25
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.04
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.37
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.80
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.42
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.15
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.18
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.03
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.33
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.79
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.38
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.16
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.05
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.07
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.50
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:28 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7219785575048734 (run: 5/10).
INFO: Testing took 0:07:33 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:07:33 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 4).
INFO: Training for epoch_poet done.
INFO: Training took 0:09:13 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7212646515278095 (run: 6/10).
INFO: Testing took 0:09:18 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:09:18 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:25 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7574491785018103 (run: 7/10).
INFO: Testing took 0:07:30 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:07:30 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']

Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.83
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.40
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.64
  Validation Loss: 0.89
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.16
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.08
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.05
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.44
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.79
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.58
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.40
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.79
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.15
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.77
  Validation Loss: 0.78
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.04
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.76
  Validation Loss: 1.26
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.01
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.76
  Validation Loss: 1.35
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.79
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.41
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.20
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.77
  Validation Loss: 0.87
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.08
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 1.16
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.82
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 0.66
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.40
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.66
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.15
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 1.18
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:24 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6802308802308803 (run: 8/10).
INFO: Testing took 0:07:29 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:07:29 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:26 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7235790038421617 (run: 9/10).
INFO: Testing took 0:07:31 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:07:31 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:27 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7757056783372573 (run: 10/10).
INFO: Testing took 0:07:32 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:07:32 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 78.23333333333333 minute(s).

Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.06
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 1.46
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.82
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.66
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.43
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.69
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.17
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.37
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.09
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.44
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.80
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.40
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.74
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.19
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 1.19
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.06
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 1.23
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________

INFO: Argument combination 6/6.
INFO: Batch size: 8.
INFO: Learning rate: 2e-05.
Using TensorFlow backend.
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
/home/paulus/.conda/envs/bertpoems/lib/python3.8/site-packages/transformers/tokenization_utils.py:884: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.
  warnings.warn(
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:32 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7602060707323864 (run: 1/10).
INFO: Testing took 0:07:36 (h:mm:ss) 

INFO: Training for run 1/10 completed.
INFO: Training run took 0:07:36 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:25 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.768194476089213 (run: 2/10).
INFO: Testing took 0:07:29 (h:mm:ss) 

INFO: Training for run 2/10 completed.
INFO: Training run took 0:07:29 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:27 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7051452933031882 (run: 3/10).
INFO: Testing took 0:07:31 (h:mm:ss) 

INFO: Training for run 3/10 completed.
INFO: Training run took 0:07:31 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:25 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7348242400873981 (run: 4/10).
INFO: Testing took 0:07:30 (h:mm:ss) 

INFO: Training for run 4/10 completed.
INFO: Training run took 0:07:30 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
There are 1 GPU(s) available.
Used GPU: GeForce RTX 2080 Ti

======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.82
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.59
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.41
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.68
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.20
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.95
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.07
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 1.37
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.82
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.39
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.61
  Validation Loss: 0.87
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.18
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.94
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.06
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 1.32
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.82
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.44
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 0.61
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.19
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.20
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.08
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.35
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.81
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.72
  Validation Loss: 0.60
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:37.
Batch   150  of    304.    Elapsed: 0:00:55.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.39
  Training epoch took: 0:01:50

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.72
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.17
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.76
  Validation Loss: 1.01
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.05
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.77
  Validation Loss: 1.12
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========INFO: Stopping epoch run early (Epoch 5).
INFO: Training for epoch_poet done.
INFO: Training took 0:11:05 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7338738259790892 (run: 5/10).
INFO: Testing took 0:11:10 (h:mm:ss) 

INFO: Training for run 5/10 completed.
INFO: Training run took 0:11:10 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:29 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7168267891952103 (run: 6/10).
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: Training for run 6/10 completed.
INFO: Training run took 0:07:34 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:25 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7506854256854257 (run: 7/10).
INFO: Testing took 0:07:30 (h:mm:ss) 

INFO: Training for run 7/10 completed.
INFO: Training run took 0:07:30 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']

Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.81
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.71
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.44
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.92
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.21
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.35
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.09
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 1.35
  Validation took: 0:00:04

======== Epoch 5 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:27.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.04
  Training epoch took: 0:01:46

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.78
  Validation took: 0:00:04

======== Epoch 6 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.01
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.65
  Validation Loss: 2.02
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.81
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.60
  Validation Loss: 0.78
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.42
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.67
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.18
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.71
  Validation Loss: 0.83
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.05
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.48
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.83
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.64
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.41
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 0.82
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:45.

  Average training loss: 0.20
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.74
  Validation Loss: 0.86
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.07
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.77
  Validation Loss: 1.08
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:34.
Batch   150  of    304.    Elapsed: 0:00:52.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.82
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.57
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.43
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.69
  Validation Loss: 0.62
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:25 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6675476570213413 (run: 8/10).
INFO: Testing took 0:07:30 (h:mm:ss) 

INFO: Training for run 8/10 completed.
INFO: Training run took 0:07:30 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:30 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.6839445642077221 (run: 9/10).
INFO: Testing took 0:07:34 (h:mm:ss) 

INFO: Training for run 9/10 completed.
INFO: Training run took 0:07:34 (h:mm:ss)
INFO: Model name '../corpora/domain-adaption/redewiedergabe-alternative/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '../corpora/domain-adaption/redewiedergabe-alternative/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO: Didn't find file ../corpora/domain-adaption/redewiedergabe-alternative/added_tokens.json. We won't load it.
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/vocab.txt
INFO: loading file None
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/special_tokens_map.json
INFO: loading file ../corpora/domain-adaption/redewiedergabe-alternative/tokenizer_config.json
INFO: loading configuration file ../corpora/domain-adaption/redewiedergabe-alternative/config.json
INFO: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

INFO: loading weights file ../corpora/domain-adaption/redewiedergabe-alternative/pytorch_model.bin
INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO: Stopping epoch run early (Epoch 3).
INFO: Training for epoch_poet done.
INFO: Training took 0:07:27 (h:mm:ss) 

INFO: Testing for epoch_poet done.
INFO: CV Test F1-Score: 0.7244671020986811 (run: 10/10).
INFO: Testing took 0:07:32 (h:mm:ss) 

INFO: Training for run 10/10 completed.
INFO: Training run took 0:07:32 (h:mm:ss)
INFO: Writing results to '../results/bert/'.
INFO: Total duration: 80.0 minute(s).

Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.18
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.75
  Validation Loss: 0.86
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:10.
Batch   250  of    304.    Elapsed: 0:01:28.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.06
  Training epoch took: 0:01:47

Now Validating.
  Validation Accuracy: 0.68
  Validation Loss: 1.45
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.81
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.55
  Validation Loss: 0.81
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:48.

  Average training loss: 0.43
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.62
  Validation Loss: 0.91
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:12.
Batch   250  of    304.    Elapsed: 0:01:30.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.18
  Training epoch took: 0:01:49

Now Validating.
  Validation Accuracy: 0.66
  Validation Loss: 1.32
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.07
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.67
  Validation Loss: 1.71
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________


======== Epoch 1 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:17.
Batch   100  of    304.    Elapsed: 0:00:35.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.84
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.63
  Validation Loss: 0.74
  Validation took: 0:00:04

======== Epoch 2 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:54.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:46.

  Average training loss: 0.45
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.70
  Validation Loss: 0.57
  Validation took: 0:00:04

======== Epoch 3 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.18
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 0.91
  Validation took: 0:00:04

======== Epoch 4 / 10 ========
Now Training.
Batch    50  of    304.    Elapsed: 0:00:18.
Batch   100  of    304.    Elapsed: 0:00:36.
Batch   150  of    304.    Elapsed: 0:00:53.
Batch   200  of    304.    Elapsed: 0:01:11.
Batch   250  of    304.    Elapsed: 0:01:29.
Batch   300  of    304.    Elapsed: 0:01:47.

  Average training loss: 0.05
  Training epoch took: 0:01:48

Now Validating.
  Validation Accuracy: 0.73
  Validation Loss: 1.28
  Validation took: 0:00:04
--------------------------------

--------------------------------

________________________________
________________________________

INFO: Overall run-time: 537.0333333333333 minute(s).
--------------------------------------------
--------------------------------------------


--------------------------------------------
--------------------------------------------


--------------------------------------------
--------------------------------------------


--------------------------------------------
--------------------------------------------


--------------------------------------------
--------------------------------------------


--------------------------------------------
--------------------------------------------


